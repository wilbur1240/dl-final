{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11836628,"sourceType":"datasetVersion","datasetId":7436504},{"sourceId":11841607,"sourceType":"datasetVersion","datasetId":7439989},{"sourceId":11845104,"sourceType":"datasetVersion","datasetId":7442332},{"sourceId":11852728,"sourceType":"datasetVersion","datasetId":7447769}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-17T08:42:05.937083Z","iopub.execute_input":"2025-05-17T08:42:05.937351Z","iopub.status.idle":"2025-05-17T08:42:06.203356Z","shell.execute_reply.started":"2025-05-17T08:42:05.937331Z","shell.execute_reply":"2025-05-17T08:42:06.202770Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/pixel-art-sprites/sprites.npy\n/kaggle/input/pixel-art-sprites/sprites_labels.npy\n/kaggle/input/model-100/Classifier_free_DDIM.pth\n/kaggle/input/pixel-art/images.tgz\n/kaggle/input/pixel-art/labels.csv\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"from typing import Dict, Tuple\nfrom tqdm import tqdm\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\nfrom torchvision import models, transforms\nfrom torchvision.utils import save_image, make_grid\nimport matplotlib.pyplot as plt\nfrom matplotlib.animation import FuncAnimation, PillowWriter\nimport numpy as np\nfrom IPython.display import HTML\nfrom torchvision.utils import save_image, make_grid\nimport os\nimport torchvision.transforms as transforms\nfrom torch.utils.data import Dataset\nfrom PIL import Image\nimport math","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T08:43:19.288754Z","iopub.execute_input":"2025-05-17T08:43:19.289179Z","iopub.status.idle":"2025-05-17T08:43:19.293767Z","shell.execute_reply.started":"2025-05-17T08:43:19.289158Z","shell.execute_reply":"2025-05-17T08:43:19.293063Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# use sinusoidal position embedding to encode time step (https://arxiv.org/abs/1706.03762)   \ndef timestep_embedding(timesteps, dim, max_period=500):\n    \"\"\"\n    Create sinusoidal timestep embeddings.\n    :param timesteps: a 1-D Tensor of N indices, one per batch element.\n                      These may be fractional.\n    :param dim: the dimension of the output.\n    :param max_period: controls the minimum frequency of the embeddings.\n    :return: an [N x dim] Tensor of positional embeddings.\n    \"\"\"\n    half = dim // 2\n    freqs = torch.exp(\n        -math.log(max_period) * torch.arange(start=0, end=half, dtype=torch.float32) / half\n    ).to(device=timesteps.device)\n    args = timesteps[:, None].float() * freqs[None]\n    embedding = torch.cat([torch.cos(args), torch.sin(args)], dim=-1)\n    if dim % 2:\n        embedding = torch.cat([embedding, torch.zeros_like(embedding[:, :1])], dim=-1)\n    return embedding","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T08:43:25.170051Z","iopub.execute_input":"2025-05-17T08:43:25.170605Z","iopub.status.idle":"2025-05-17T08:43:25.175449Z","shell.execute_reply.started":"2025-05-17T08:43:25.170582Z","shell.execute_reply":"2025-05-17T08:43:25.174757Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"class EmbedFC(nn.Module):\n    def __init__(self, input_dim, emb_dim):\n        super(EmbedFC, self).__init__()\n        '''\n        This class defines a generic one layer feed-forward neural network for embedding input data of\n        dimensionality input_dim to an embedding space of dimensionality emb_dim.\n        '''\n        self.input_dim = input_dim\n        \n        # define the layers for the network\n        layers = [\n            nn.Linear(input_dim, emb_dim),\n            nn.GELU(),\n            nn.Linear(emb_dim, emb_dim),\n        ]\n        \n        # create a PyTorch sequential model consisting of the defined layers\n        self.model = nn.Sequential(*layers)\n\n    def forward(self, x):\n        # flatten the input tensor\n        x = x.view(-1, self.input_dim)\n        # apply the model layers to the flattened tensor\n        return self.model(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T08:43:28.072956Z","iopub.execute_input":"2025-05-17T08:43:28.073209Z","iopub.status.idle":"2025-05-17T08:43:28.078145Z","shell.execute_reply.started":"2025-05-17T08:43:28.073189Z","shell.execute_reply":"2025-05-17T08:43:28.077431Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"class ContextEmbeddingBlock(nn.Module):\n    def __init__(self, time_dim, context_dim, embed_dim):\n        super().__init__()\n        self.contextembed4 = EmbedFC(context_dim, 4 * embed_dim)\n        self.contextembed3 = EmbedFC(context_dim, 2 * embed_dim)\n        #self.contextembed0 = EmbedFC(context_dim, 8 * embed_dim)\n        self.timeembed4 = EmbedFC(time_dim, 4 * embed_dim)\n        self.timeembed3 = EmbedFC(time_dim, 2 * embed_dim)\n        #self.timeembed0 = EmbedFC(time_dim, 8 * embed_dim)\n    def forward(self, t_sin, c, device ,mask_c=None):\n        c = c.float()\n        if c is None:\n            c = torch.zeros(t_sin.size(0), self.contextembed1.in_features).to(device)\n        if mask_c is not None:\n            c = c * mask_c.view(-1, 1)  # 遮蔽 c 條件向量\n        \n        cemb4 = self.contextembed4(c).view(t_sin.size(0), -1, 1, 1)\n        temb4 = self.timeembed4(t_sin).view(t_sin.size(0), -1, 1, 1)\n        cemb3 = self.contextembed3(c).view(t_sin.size(0), -1, 1, 1)\n        temb3 = self.timeembed3(t_sin).view(t_sin.size(0), -1, 1, 1)\n        return cemb4, temb4, cemb3, temb3","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T08:47:19.047534Z","iopub.execute_input":"2025-05-17T08:47:19.048027Z","iopub.status.idle":"2025-05-17T08:47:19.054318Z","shell.execute_reply.started":"2025-05-17T08:47:19.048004Z","shell.execute_reply":"2025-05-17T08:47:19.053756Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"class SelfAttention2d(nn.Module):\n    def __init__(self, dim, num_heads=4):\n        super().__init__()\n        self.norm = nn.BatchNorm2d(dim)\n        self.attn = nn.MultiheadAttention(embed_dim=dim, num_heads=num_heads, batch_first=True)\n        self.proj = nn.Linear(dim, dim)\n\n    def forward(self, x):\n        # x: [B, C, H, W] → flatten to [B, HW, C]\n        B, C, H, W = x.shape\n        x_ = self.norm(x).view(B, C, H*W).permute(0, 2, 1)  # [B, HW, C]\n        attn_out, _ = self.attn(x_, x_, x_)\n        attn_out = self.proj(attn_out)\n        out = attn_out + x_  # skip connection\n        out = out.permute(0, 2, 1).view(B, C, H, W)\n        return out","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T08:43:33.092145Z","iopub.execute_input":"2025-05-17T08:43:33.092924Z","iopub.status.idle":"2025-05-17T08:43:33.100225Z","shell.execute_reply.started":"2025-05-17T08:43:33.092894Z","shell.execute_reply":"2025-05-17T08:43:33.099549Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"class ResidualConvBlock(nn.Module):\n    def __init__(\n        self, in_channels: int, out_channels: int, is_res: bool = False\n    ) -> None:\n        super().__init__()\n\n        #檢查in out 通道數是否相同\n        self.same_channels = in_channels == out_channels\n\n        # Flag for whether or not to use residual connection\n        self.is_res = is_res\n\n        # First convolutional layer\n        self.conv1 = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, 3, 1, 1),   # 3x3 kernel with stride 1 and padding 1\n            nn.BatchNorm2d(out_channels),   # Batch normalization\n            nn.GELU(),   # GELU activation function\n        )\n\n        # Second convolutional layer\n        self.conv2 = nn.Sequential(\n            nn.Conv2d(out_channels, out_channels, 3, 1, 1),   # 3x3 kernel with stride 1 and padding 1\n            nn.BatchNorm2d(out_channels),   # Batch normalization\n            nn.GELU(),   # GELU activation function\n        )\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n\n        # If using residual connection\n        if self.is_res:\n            # Apply first convolutional layer\n            x1 = self.conv1(x)\n\n            # Apply second convolutional layer\n            x2 = self.conv2(x1)\n\n            # If input and output channels are the same, add residual connection directly\n            if self.same_channels:\n                out = x + x2\n            else:\n                # If not, apply a 1x1 convolutional layer to match dimensions before adding residual connection\n                shortcut = nn.Conv2d(x.shape[1], x2.shape[1], kernel_size=1, stride=1, padding=0).to(x.device)\n                out = shortcut(x) + x2\n            #print(f\"resconv forward: x {x.shape}, x1 {x1.shape}, x2 {x2.shape}, out {out.shape}\")\n\n            # Normalize output tensor\n            return out / 1.414\n\n        # If not using residual connection, return output of second convolutional layer\n        else:\n            x1 = self.conv1(x)\n            x2 = self.conv2(x1)\n            return x2\n\n    # Method to get the number of output channels for this block\n    def get_out_channels(self):\n        return self.conv2[0].out_channels\n\n    # Method to set the number of output channels for this block\n    def set_out_channels(self, out_channels):\n        self.conv1[0].out_channels = out_channels\n        self.conv2[0].in_channels = out_channels\n        self.conv2[0].out_channels = out_channels","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T08:43:35.339308Z","iopub.execute_input":"2025-05-17T08:43:35.339585Z","iopub.status.idle":"2025-05-17T08:43:35.347097Z","shell.execute_reply.started":"2025-05-17T08:43:35.339564Z","shell.execute_reply":"2025-05-17T08:43:35.346409Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"class UnetUp(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(UnetUp, self).__init__()\n        \n        # Create a list of layers for the upsampling block\n        # The block consists of a ConvTranspose2d layer for upsampling, followed by two ResidualConvBlock layers\n        layers = [\n            nn.ConvTranspose2d(in_channels, out_channels, 2, 2),\n            ResidualConvBlock(out_channels, out_channels),\n            ResidualConvBlock(out_channels, out_channels),\n        ]\n        \n        # Use the layers to create a sequential model\n        self.model = nn.Sequential(*layers)\n\n    def forward(self, x, skip):\n        # Concatenate the input tensor x with the skip connection tensor along the channel dimension\n        x = torch.cat((x, skip), 1)\n        \n        # Pass the concatenated tensor through the sequential model and return the output\n        x = self.model(x)\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T08:43:38.772166Z","iopub.execute_input":"2025-05-17T08:43:38.772410Z","iopub.status.idle":"2025-05-17T08:43:38.778124Z","shell.execute_reply.started":"2025-05-17T08:43:38.772394Z","shell.execute_reply":"2025-05-17T08:43:38.777509Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"class UnetDown(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(UnetDown, self).__init__()\n        \n        # Create a list of layers for the downsampling block\n        # Each block consists of two ResidualConvBlock layers, followed by a MaxPool2d layer for downsampling\n        layers = [ResidualConvBlock(in_channels, out_channels), ResidualConvBlock(out_channels, out_channels), nn.MaxPool2d(2)]\n        \n        # Use the layers to create a sequential model\n        self.model = nn.Sequential(*layers)\n\n    def forward(self, x):\n        # Pass the input through the sequential model and return the output\n        return self.model(x)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T08:43:41.359926Z","iopub.execute_input":"2025-05-17T08:43:41.360386Z","iopub.status.idle":"2025-05-17T08:43:41.364518Z","shell.execute_reply.started":"2025-05-17T08:43:41.360363Z","shell.execute_reply":"2025-05-17T08:43:41.363887Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"def unorm(x):\n    # unity norm. results in range of [0,1]\n    # assume x (h,w,3)\n    xmax = x.max((0,1))\n    xmin = x.min((0,1))\n    return(x - xmin)/(xmax - xmin)\n\ndef norm_all(store, n_t, n_s):\n    # runs unity norm on all timesteps of all samples\n    nstore = np.zeros_like(store)\n    for t in range(n_t):\n        for s in range(n_s):\n            nstore[t,s] = unorm(store[t,s])\n    return nstore\n\ndef norm_torch(x_all):\n    # runs unity norm on all timesteps of all samples\n    # input is (n_samples, 3,h,w), the torch image format\n    x = x_all.cpu().numpy()\n    xmax = x.max((2,3))\n    xmin = x.min((2,3))\n    xmax = np.expand_dims(xmax,(2,3)) \n    xmin = np.expand_dims(xmin,(2,3))\n    nstore = (x - xmin)/(xmax - xmin)\n    return torch.from_numpy(nstore)\n\ndef gen_tst_context(n_cfeat):\n    \"\"\"\n    Generate test context vectors(class = 5)\n    \"\"\"\n    vec = torch.tensor([\n    [1,0,0,0,0], [0,1,0,0,0], [0,0,1,0,0], [0,0,0,1,0], [0,0,0,0,1],  [0,0,0,0,0],      # human, non-human, food, spell, side-facing\n    [1,0,0,0,0], [0,1,0,0,0], [0,0,1,0,0], [0,0,0,1,0], [0,0,0,0,1],  [0,0,0,0,0],      # human, non-human, food, spell, side-facing\n    [1,0,0,0,0], [0,1,0,0,0], [0,0,1,0,0], [0,0,0,1,0], [0,0,0,0,1],  [0,0,0,0,0],      # human, non-human, food, spell, side-facing\n    [1,0,0,0,0], [0,1,0,0,0], [0,0,1,0,0], [0,0,0,1,0], [0,0,0,0,1],  [0,0,0,0,0],      # human, non-human, food, spell, side-facing\n    [1,0,0,0,0], [0,1,0,0,0], [0,0,1,0,0], [0,0,0,1,0], [0,0,0,0,1],  [0,0,0,0,0],      # human, non-human, food, spell, side-facing\n    [1,0,0,0,0], [0,1,0,0,0], [0,0,1,0,0], [0,0,0,1,0], [0,0,0,0,1],  [0,0,0,0,0]]      # human, non-human, food, spell, side-facing\n    )\n    return len(vec), vec\n\ndef plot_grid(x,n_sample,n_rows,save_dir,w):\n    # x:(n_sample, 3, h, w)\n    ncols = n_sample//n_rows\n    grid = make_grid(norm_torch(x), nrow=ncols)  # curiously, nrow is number of columns.. or number of items in the row.\n    save_image(grid, save_dir + f\"run_image_w{w}.png\")\n    print('saved image at ' + save_dir + f\"run_image_w{w}.png\")\n    return grid\n\ndef plot_sample(x_gen_store,n_sample,nrows,save_dir, fn,  w, save=False):\n    ncols = n_sample//nrows\n    sx_gen_store = np.moveaxis(x_gen_store,2,4)                               # change to Numpy image format (h,w,channels) vs (channels,h,w)\n    nsx_gen_store = norm_all(sx_gen_store, sx_gen_store.shape[0], n_sample)   # unity norm to put in range [0,1] for np.imshow\n    \n    # create gif of images evolving over time, based on x_gen_store\n    fig, axs = plt.subplots(nrows=nrows, ncols=ncols, sharex=True, sharey=True,figsize=(ncols,nrows))\n    def animate_diff(i, store):\n        print(f'gif animating frame {i} of {store.shape[0]}', end='\\r')\n        plots = []\n        for row in range(nrows):\n            for col in range(ncols):\n                axs[row, col].clear()\n                axs[row, col].set_xticks([])\n                axs[row, col].set_yticks([])\n                plots.append(axs[row, col].imshow(store[i,(row*ncols)+col]))\n        return plots\n    ani = FuncAnimation(fig, animate_diff, fargs=[nsx_gen_store],  interval=200, blit=False, repeat=True, frames=nsx_gen_store.shape[0]) \n    plt.close()\n    if save:\n        ani.save(save_dir + f\"{fn}_w{w}.gif\", dpi=100, writer=PillowWriter(fps=5))\n        print('saved gif at ' + save_dir + f\"{fn}_w{w}.gif\")\n    return ani","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T08:43:43.634914Z","iopub.execute_input":"2025-05-17T08:43:43.635586Z","iopub.status.idle":"2025-05-17T08:43:43.648285Z","shell.execute_reply.started":"2025-05-17T08:43:43.635564Z","shell.execute_reply":"2025-05-17T08:43:43.647506Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"class CustomDataset(Dataset):\n    def __init__(self, sfilename, lfilename, transform, drop_p=0.1, null_context=False):\n        self.sprites = np.load(sfilename)\n        self.slabels = np.load(lfilename)\n        print(f\"sprite shape: {self.sprites.shape}\")\n        print(f\"labels shape: {self.slabels.shape}\")\n        self.transform = transform\n        self.null_context = null_context\n        self.sprites_shape = self.sprites.shape\n        self.slabel_shape = self.slabels.shape\n        self.drop_p = drop_p\n        \n    # Return the number of images in the dataset\n    def __len__(self):\n        return len(self.sprites)\n    \n    # Get the image and label at a given index\n    def __getitem__(self, idx):\n        # Return the image and label as a tuple\n        if self.transform:\n            image = self.transform(self.sprites[idx])\n            label = torch.tensor(self.slabels[idx]).to(torch.int64)\n        return (image, label)\n\n    def getshapes(self):\n        # return shapes of data and labels\n        return self.sprites_shape, self.slabel_shape\n\ntransform = transforms.Compose([\n    transforms.ToTensor(),                # from [0,255] to range [0.0,1.0]\n    transforms.Normalize((0.5,), (0.5,))  # range [-1,1]\n\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T08:43:46.430209Z","iopub.execute_input":"2025-05-17T08:43:46.430775Z","iopub.status.idle":"2025-05-17T08:43:46.436557Z","shell.execute_reply.started":"2025-05-17T08:43:46.430753Z","shell.execute_reply":"2025-05-17T08:43:46.435882Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"class ContextUnet(nn.Module):\n    def __init__(self, in_channels, n_feat=256, n_cfeat=10, height=28):\n        super(ContextUnet, self).__init__()\n        self.in_channels = in_channels\n        self.n_feat = n_feat\n        self.n_cfeat = n_cfeat\n        self.h = height\n\n        # Encoder\n        self.init_conv = ResidualConvBlock(in_channels, n_feat, is_res=True)\n        \n        self.down1 = UnetDown(n_feat, n_feat) #64\n        self.down2 = UnetDown(n_feat, 2 * n_feat) #128\n        \n        self.down3 = nn.Sequential(\n            UnetDown(2 * n_feat, 4 * n_feat),\n            SelfAttention2d(4 * n_feat)\n        )\n        \n        #self.to_vec = nn.Sequential(nn.AvgPool2d((4)), nn.GELU())\n        \n        self.to_vec = nn.Sequential(\n            nn.AvgPool2d((2)),\n            ResidualConvBlock(4 * n_feat, 4 * n_feat),\n            SelfAttention2d(4 * n_feat),\n            nn.GELU()\n        )\n        # Embedding block (時間 + 條件)\n        self.embed = ContextEmbeddingBlock(n_feat, n_cfeat, n_feat)\n        \n        # Decoder\n        self.up0 = nn.Sequential(\n            nn.ConvTranspose2d(4 * n_feat, 4 * n_feat, kernel_size=2, stride=2),\n            nn.GroupNorm(8, 4 * n_feat),\n            nn.ReLU(),\n            SelfAttention2d(4 * n_feat),\n        )\n        \n        self.up3 = UnetUp(8 * n_feat, 2 * n_feat) # 4n → 2n\n        self.up2 = UnetUp(4 * n_feat, n_feat) # 2n → 1n\n        self.up1 = UnetUp(2 * n_feat, n_feat) # 1n → 1n\n        \n        self.attn1 =  SelfAttention2d(n_feat)\n        self.attn2 =  SelfAttention2d(n_feat)\n        \n        # Output\n        self.out = nn.Sequential(\n            nn.Conv2d(2 * n_feat, n_feat, 3, 1, 1),\n            nn.GroupNorm(8, n_feat),\n            nn.ReLU(),\n            nn.Conv2d(n_feat, self.in_channels, 3, 1, 1),\n        )\n        self.x_proj = nn.Conv2d(in_channels, n_feat, kernel_size=1)\n        self.out_attn = SelfAttention2d(n_feat)\n    def forward(self, x, t, c=None, mask_c=None):\n        x_in = x\n        x_proj = self.x_proj(x_in)\n        x = self.init_conv(x) #64\n        d1 = self.down1(x) #64\n        d2 = self.down2(d1) #128\n        d3 = self.down3(d2) #256\n        hvec = self.to_vec(d3) #256\n        # 時間 & 條件 embedding\n        t_sin = timestep_embedding(t.squeeze(-1), dim=self.n_feat, max_period=10000)\n        #cemb4, temb4, cemb3, temb3, cemb0, temb0 = self.embed(t_sin, c, x.device,mask_c)\n        cemb0, temb0, cemb3, temb3  = self.embed(t_sin, c, x.device,mask_c)\n        # 上採樣過程 + attention\n        up0 = self.up0(hvec) #256\n        cemb0 = F.interpolate(cemb0, size=up0.shape[-2:], mode='nearest')  # or 'bilinear'\n        temb0 = F.interpolate(temb0, size=up0.shape[-2:], mode='nearest')\n        up3 = self.up3(cemb0 * up0 + temb0, d3) #128\n        cemb3 = F.interpolate(cemb3, size=up3.shape[-2:], mode='nearest')  # or 'bilinear'\n        temb3 = F.interpolate(temb3, size=up3.shape[-2:], mode='nearest')\n        up2 = self.up2(cemb3 * up3 + temb3, d2) #64\n        up2 = self.attn2(up2) #64\n        up1 = self.up1(up2, d1) #64\n        up1 = self.attn1(up1) #64\n        concat = torch.cat((up1, x_proj), dim=1)\n        out = self.out(concat)\n        return out","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T08:43:49.638758Z","iopub.execute_input":"2025-05-17T08:43:49.639008Z","iopub.status.idle":"2025-05-17T08:43:49.650317Z","shell.execute_reply.started":"2025-05-17T08:43:49.638992Z","shell.execute_reply":"2025-05-17T08:43:49.649537Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"# hyperparameters\n\n# diffusion hyperparameters\ntimesteps = 500\nBeta1 = 2e-4\nBeta2 = 0.05\n\n# network hyperparameters\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else torch.device('cpu'))\nn_feat = 64 # 64 hidden dimension feature\nn_cfeat = 5 # context vector is of size 5\nheight = 16 # 16x16 image\nsave_dir = '/kaggle/working/'\n\n# training hyperparameters\nbatch_size = 100\nlrate= 5e-4\nn_class = 5","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T08:43:53.037411Z","iopub.execute_input":"2025-05-17T08:43:53.037953Z","iopub.status.idle":"2025-05-17T08:43:53.095522Z","shell.execute_reply.started":"2025-05-17T08:43:53.037928Z","shell.execute_reply":"2025-05-17T08:43:53.094767Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"# beta schedule\ndef linear_beta_schedule(timesteps):\n    scale = 1000 / timesteps\n    beta1 = scale * Beta1\n    beta2  = scale * Beta2\n    b_t = (beta2 - beta1) * torch.linspace(0, 1, timesteps + 1, device=device) + beta1\n    return b_t[1:]\n\ndef sigmoid_beta_schedule(timesteps):\n    betas = torch.linspace(-6, 6, timesteps)\n    betas = torch.sigmoid(betas)/(betas.max()-betas.min())*(0.02-betas.min())/10\n    return betas\n\ndef cosine_beta_schedule(timesteps, s=0.008):\n    \"\"\"\n    cosine schedule\n    as proposed in https://arxiv.org/abs/2102.09672\n    \"\"\"\n    steps = timesteps + 1\n    x = torch.linspace(0, timesteps, steps, dtype=torch.float64)\n    alphas_cumprod = torch.cos(((x / timesteps) + s) / (1 + s) * math.pi * 0.5) ** 2\n    alphas_cumprod = alphas_cumprod / alphas_cumprod[0]\n    betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])\n    return torch.clip(betas, 0, 0.999)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T08:43:55.532206Z","iopub.execute_input":"2025-05-17T08:43:55.532933Z","iopub.status.idle":"2025-05-17T08:43:55.538518Z","shell.execute_reply.started":"2025-05-17T08:43:55.532907Z","shell.execute_reply":"2025-05-17T08:43:55.537765Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"class GaussianDiffusion:\n    def __init__(\n        self,\n        timesteps=500,\n        beta_schedule='linear'\n    ):\n        self.timesteps = timesteps\n        \n        if beta_schedule == 'linear':\n            betas = linear_beta_schedule(timesteps)\n        elif beta_schedule == 'cosine':\n            betas = cosine_beta_schedule(timesteps)\n        elif beta_schedule == 'sigmoid':\n            betas = sigmoid_beta_schedule(timesteps)\n        else:\n            raise ValueError(f'unknown beta schedule {beta_schedule}')\n        self.betas = betas\n            \n        self.alphas = 1. - self.betas\n        self.alphas_cumprod = torch.cumprod(self.alphas, axis=0)\n        self.alphas_cumprod_prev = F.pad(self.alphas_cumprod[:-1], (1, 0), value=1.)\n        \n        # calculations for diffusion q(x_t | x_{t-1}) and others\n        self.sqrt_alphas_cumprod = torch.sqrt(self.alphas_cumprod)\n        self.sqrt_one_minus_alphas_cumprod = torch.sqrt(1.0 - self.alphas_cumprod)\n        self.log_one_minus_alphas_cumprod = torch.log(1.0 - self.alphas_cumprod)\n        self.sqrt_recip_alphas_cumprod = torch.sqrt(1.0 / self.alphas_cumprod)\n        self.sqrt_recipm1_alphas_cumprod = torch.sqrt(1.0 / self.alphas_cumprod - 1)\n        \n        # calculations for posterior q(x_{t-1} | x_t, x_0)\n        self.posterior_variance = (\n            self.betas * (1.0 - self.alphas_cumprod_prev) / (1.0 - self.alphas_cumprod)\n        )\n        # below: log calculation clipped because the posterior variance is 0 at the beginning\n        # of the diffusion chain\n        #self.posterior_log_variance_clipped = torch.log(self.posterior_variance.clamp(min =1e-20))\n        self.posterior_log_variance_clipped = torch.log(\n            torch.cat([self.posterior_variance[1:2], self.posterior_variance[1:]])\n        )\n        \n        self.posterior_mean_coef1 = (\n            self.betas * torch.sqrt(self.alphas_cumprod_prev) / (1.0 - self.alphas_cumprod)\n        )\n        self.posterior_mean_coef2 = (\n            (1.0 - self.alphas_cumprod_prev)\n            * torch.sqrt(self.alphas)\n            / (1.0 - self.alphas_cumprod)\n        )\n    \n    # get the param of given timestep t\n    def _extract(self, a, t, x_shape):\n        batch_size = t.shape[0]\n        out = a.to(t.device).gather(0, t).float()\n        out = out.reshape(batch_size, *((1,) * (len(x_shape) - 1)))\n        return out\n    \n    # forward diffusion (using the nice property): q(x_t | x_0)\n    def q_sample(self, x_start, t, noise=None):\n        if noise is None:\n            noise = torch.randn_like(x_start)\n\n        sqrt_alphas_cumprod_t = self._extract(self.sqrt_alphas_cumprod, t, x_start.shape)\n        sqrt_one_minus_alphas_cumprod_t = self._extract(self.sqrt_one_minus_alphas_cumprod, t, x_start.shape)\n\n        return sqrt_alphas_cumprod_t * x_start + sqrt_one_minus_alphas_cumprod_t * noise\n    \n    # Get the mean and variance of q(x_t | x_0).\n    def q_mean_variance(self, x_start, t):\n        mean = self._extract(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start\n        variance = self._extract(1.0 - self.alphas_cumprod, t, x_start.shape)\n        log_variance = self._extract(self.log_one_minus_alphas_cumprod, t, x_start.shape)\n        return mean, variance, log_variance\n    \n    # Compute the mean and variance of the diffusion posterior: q(x_{t-1} | x_t, x_0)\n    def q_posterior_mean_variance(self, x_start, x_t, t):\n        posterior_mean = (\n            self._extract(self.posterior_mean_coef1, t, x_t.shape) * x_start\n            + self._extract(self.posterior_mean_coef2, t, x_t.shape) * x_t\n        )\n        posterior_variance = self._extract(self.posterior_variance, t, x_t.shape)\n        posterior_log_variance_clipped = self._extract(self.posterior_log_variance_clipped, t, x_t.shape)\n        return posterior_mean, posterior_variance, posterior_log_variance_clipped\n    \n    # compute x_0 from x_t and pred noise: the reverse of `q_sample`\n    def predict_start_from_noise(self, x_t, t, noise):\n        return (\n            self._extract(self.sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t -\n            self._extract(self.sqrt_recipm1_alphas_cumprod, t, x_t.shape) * noise\n        )\n    \n    # compute predicted mean and variance of p(x_{t-1} | x_t)\n    def p_mean_variance(self, model, x_t, t, c, w, clip_denoised=True):\n        device = next(model.parameters()).device\n        batch_size = x_t.shape[0]\n        # predict noise using model\n        pred_noise_c = model(x_t, t, c, torch.ones(batch_size).int().to(device))\n        pred_noise_none = model(x_t, t, c, torch.zeros(batch_size).int().to(device))\n        pred_noise = (1+w)*pred_noise_c - w*pred_noise_none\n        \n        # get the predicted x_0: different from the algorithm2 in the paper\n        x_recon = self.predict_start_from_noise(x_t, t, pred_noise)\n        if clip_denoised:\n            x_recon = torch.clamp(x_recon, min=-1., max=1.)\n        model_mean, posterior_variance, posterior_log_variance = \\\n                    self.q_posterior_mean_variance(x_recon, x_t, t)\n        return model_mean, posterior_variance, posterior_log_variance\n        \n    # denoise_step: sample x_{t-1} from x_t and pred_noise\n    @torch.no_grad()\n    def p_sample(self, model, x_t, t, c, w, clip_denoised=True):\n        # predict mean and variance\n        model_mean, _, model_log_variance = self.p_mean_variance(model, x_t, t,\n                                                c, w, clip_denoised=clip_denoised)\n        noise = torch.randn_like(x_t)\n        # no noise when t == 0\n        nonzero_mask = ((t != 0).float().view(-1, *([1] * (len(x_t.shape) - 1))))\n        # compute x_{t-1}= μ + σ * ε\n        pred_img = model_mean + nonzero_mask * (0.5 * model_log_variance).exp() * noise\n        return pred_img\n    \n    # denoise: reverse diffusion\n    @torch.no_grad()\n    def p_sample_loop(self, model, shape, n_class=5, w=2, mode='random', clip_denoised=True):\n        batch_size = shape[0]\n        device = next(model.parameters()).device\n        \n        # generate labels\n        if mode == 'random':\n            cur_y = torch.randint(0, n_class, (batch_size,)).to(device)\n        elif mode == 'all':\n            if batch_size%n_class!=0:\n                batch_size = n_class\n                print('change batch_size to', n_class)\n            cur_y = torch.tensor([x for x in range(n_class)]*(batch_size//n_class), dtype=torch.long).to(device)\n        else:\n            cur_y = torch.ones(batch_size).long().to(device)*int(mode)\n\n        cur_y = F.one_hot(cur_y, num_classes=n_class).float().to(device)\n        # start from pure noise (for each example in the batch)\n        img = torch.randn(shape, device=device)\n        imgs = []\n        for i in tqdm(reversed(range(0, timesteps)), desc='sampling loop time step', total=timesteps):\n            img = self.p_sample(model, img, torch.full((batch_size,), i, device=device, dtype=torch.long), cur_y, w, clip_denoised)\n            imgs.append(img.cpu().numpy())\n        return imgs\n    \n    # sample new images\n    @torch.no_grad()\n    def sample(self, model, image_size, batch_size=100, channels=3, n_class=5, w=2, mode='random', clip_denoised=True):\n        return self.p_sample_loop(model, (batch_size, channels, image_size, image_size), n_class, w, mode, clip_denoised)\n    \n    # use ddim to sample\n    @torch.no_grad()\n    def ddim_sample(\n        self,\n        model,\n        image_size,\n        batch_size=100,\n        channels=3,\n        ddim_timesteps=50,\n        n_class = 5,\n        w = 2.0,\n        mode= 'random',\n        ddim_discr_method=\"uniform\",\n        ddim_eta=0.0,\n        clip_denoised=True):\n        # make ddim timestep sequence\n        if ddim_discr_method == 'uniform':\n            c = self.timesteps // ddim_timesteps\n            ddim_timestep_seq = np.asarray(list(range(0, self.timesteps, c)))\n        elif ddim_discr_method == 'quad':\n            ddim_timestep_seq = (\n                (np.linspace(0, np.sqrt(self.timesteps * .8), ddim_timesteps)) ** 2\n            ).astype(int)\n        else:\n            raise NotImplementedError(f'There is no ddim discretization method called \"{ddim_discr_method}\"')\n        # add one to get the final alpha values right (the ones from first scale to data during sampling)\n        ddim_timestep_seq = ddim_timestep_seq + 1\n        # previous sequence\n        ddim_timestep_prev_seq = np.append(np.array([0]), ddim_timestep_seq[:-1])\n        \n        device = next(model.parameters()).device\n        \n        # generate labels\n        if mode == 'random':\n            cur_y = torch.randint(0, n_class, (batch_size,)).to(device)\n        elif mode == 'all':\n            if batch_size%n_class!=0:\n                batch_size = n_class\n                print('change batch_size to', n_class)\n            cur_y = torch.tensor([x for x in range(n_class)]*(batch_size//n_class), dtype=torch.long).to(device)\n        else:\n            cur_y = torch.ones(batch_size).long().to(device)*int(mode)\n        cur_y = F.one_hot(cur_y, num_classes=n_class).float().to(device)\n        # start from pure noise (for each example in the batch)\n        sample_img = torch.randn((batch_size, channels, image_size, image_size), device=device)\n        seq_img = [sample_img.cpu().numpy()]\n        \n        for i in tqdm(reversed(range(0, ddim_timesteps)), desc='sampling loop time step', total=ddim_timesteps):\n            t = torch.full((batch_size,), ddim_timestep_seq[i], device=device, dtype=torch.long)\n            prev_t = torch.full((batch_size,), ddim_timestep_prev_seq[i], device=device, dtype=torch.long)\n            \n            # 1. get current and previous alpha_cumprod\n            alpha_cumprod_t = self._extract(self.alphas_cumprod, t, sample_img.shape)\n            alpha_cumprod_t_prev = self._extract(self.alphas_cumprod, prev_t, sample_img.shape)\n    \n            # 2. predict noise using model\n            pred_noise_c = model(sample_img, t, cur_y, torch.ones(batch_size).int().cuda())\n            pred_noise_none = model(sample_img, t, cur_y, torch.zeros(batch_size).int().cuda())\n            pred_noise = (1+w)*pred_noise_c - w*pred_noise_none\n            \n            # 3. get the predicted x_0\n            pred_x0 = (sample_img - torch.sqrt((1. - alpha_cumprod_t)) * pred_noise) / torch.sqrt(alpha_cumprod_t)\n            if clip_denoised:\n                pred_x0 = torch.clamp(pred_x0, min=-1., max=1.)\n            \n            # 4. compute variance: \"sigma_t(η)\" -> see formula (16)\n            # σ_t = sqrt((1 − α_t−1)/(1 − α_t)) * sqrt(1 − α_t/α_t−1)\n            sigmas_t = ddim_eta * torch.sqrt(\n                (1 - alpha_cumprod_t_prev) / (1 - alpha_cumprod_t) * (1 - alpha_cumprod_t / alpha_cumprod_t_prev))\n            \n            # 5. compute \"direction pointing to x_t\" of formula (12)\n            pred_dir_xt = torch.sqrt(1 - alpha_cumprod_t_prev - sigmas_t**2) * pred_noise\n            \n            # 6. compute x_{t-1} of formula (12)\n            x_prev = torch.sqrt(alpha_cumprod_t_prev) * pred_x0 + pred_dir_xt + sigmas_t * torch.randn_like(sample_img)\n\n            sample_img = x_prev\n            if mode == 'all':\n                seq_img.append(sample_img.cpu().numpy())\n            \n        if mode == 'all':\n            return seq_img\n        else:\n            return sample_img.cpu().numpy()\n    \n    # compute train losses\n    def train_losses(self, model, x_start, t, c, mask_c):\n        # generate random noise\n        noise = torch.randn_like(x_start)\n        # get x_t\n        x_noisy = self.q_sample(x_start, t, noise=noise)\n        predicted_noise = model(x_noisy, t, c, mask_c)\n        loss = F.mse_loss(noise, predicted_noise)\n        return loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T08:43:58.064833Z","iopub.execute_input":"2025-05-17T08:43:58.065064Z","iopub.status.idle":"2025-05-17T08:43:58.090907Z","shell.execute_reply.started":"2025-05-17T08:43:58.065050Z","shell.execute_reply":"2025-05-17T08:43:58.090324Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"timesteps = 500\n\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.5], std=[0.5])\n])\n\n# use MNIST dataset\ndataset = CustomDataset(\"/kaggle/input/pixel-art-sprites/sprites.npy\", \"/kaggle/input/pixel-art-sprites/sprites_labels.npy\", transform, null_context=False)\ndataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=1)\n\n\n# define model and diffusion\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nnn_model = ContextUnet(\n    in_channels=3,\n    n_feat=n_feat,\n    n_cfeat=n_cfeat,\n    height=height\n)\nnn_model.to(device)\n\ngaussian_diffusion = GaussianDiffusion(timesteps=timesteps)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T08:44:04.107769Z","iopub.execute_input":"2025-05-17T08:44:04.108028Z","iopub.status.idle":"2025-05-17T08:44:05.314988Z","shell.execute_reply.started":"2025-05-17T08:44:04.108008Z","shell.execute_reply":"2025-05-17T08:44:05.314321Z"}},"outputs":[{"name":"stdout","text":"sprite shape: (89400, 16, 16, 3)\nlabels shape: (89400, 5)\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"import time\n# train\nepochs = 300\np_uncound = 0.2\n'''\nlen_data = len(dataloader)\ntime_end = time.time()\n\noptimizer = torch.optim.Adam(nn_model.parameters(), lr=lrate)\n\nsave_dir = \"./checkpoints\"\nos.makedirs(save_dir, exist_ok=True)  # 確保資料夾存在\n\nfor epoch in range(epochs):\n    for step, (images, labels) in enumerate(dataloader):     \n        time_start = time_end\n        \n        optimizer.zero_grad()\n        \n        batch_size = images.shape[0]\n        images = images.to(device)\n        labels = labels.to(device)\n        \n        # random generate mask\n        z_uncound = torch.rand(batch_size)\n        batch_mask = (z_uncound>p_uncound).float().to(device)\n        \n        # sample t uniformally for every example in the batch\n        t = torch.randint(0, timesteps, (batch_size,), device=device).long()\n        \n        loss = gaussian_diffusion.train_losses(nn_model, images, t, labels, batch_mask)\n        \n        if step % 100 == 0:\n            time_end = time.time()\n            print(\"Epoch{}/{}\\t  Step{}/{}\\t Loss {:.4f}\\t Time {:.2f}\".format(epoch+1, epochs, step+1, len_data, loss.item(), time_end-time_start))\n            \n        loss.backward()\n        optimizer.step()\n    if epoch % 50 == 0:\n        save_path = os.path.join(save_dir, f\"model_epoch_{epoch}.pt\")\n        torch.save({\n            'epoch': epoch,\n            'model_state_dict': nn_model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n        }, save_path)\n        print(f\"✅ Saved model at epoch {epoch} → {save_path}\")\n'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T08:44:07.933488Z","iopub.execute_input":"2025-05-17T08:44:07.934160Z","iopub.status.idle":"2025-05-17T08:44:07.941398Z","shell.execute_reply.started":"2025-05-17T08:44:07.934135Z","shell.execute_reply":"2025-05-17T08:44:07.940747Z"}},"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"'\\nfor epoch in range(epochs):\\n    for step, (images, labels) in enumerate(dataloader):     \\n        time_start = time_end\\n        \\n        optimizer.zero_grad()\\n        \\n        batch_size = images.shape[0]\\n        images = images.to(device)\\n        labels = labels.to(device)\\n        \\n        # random generate mask\\n        z_uncound = torch.rand(batch_size)\\n        batch_mask = (z_uncound>p_uncound).float().to(device)\\n        \\n        # sample t uniformally for every example in the batch\\n        t = torch.randint(0, timesteps, (batch_size,), device=device).long()\\n        \\n        loss = gaussian_diffusion.train_losses(nn_model, images, t, labels, batch_mask)\\n        \\n        if step % 100 == 0:\\n            time_end = time.time()\\n            print(\"Epoch{}/{}\\t  Step{}/{}\\t Loss {:.4f}\\t Time {:.2f}\".format(epoch+1, epochs, step+1, len_data, loss.item(), time_end-time_start))\\n            \\n        loss.backward()\\n        optimizer.step()\\n    if not os.path.exists(\\'./saved_models\\'):\\n        os.mkdir(\\'./saved_models\\')\\n    torch.save(nn_model, \\'./saved_models/Classifier_free_DDIM.pth\\')\\n'"},"metadata":{}}],"execution_count":25},{"cell_type":"code","source":"#torch.save(nn_model, './saved_models/Classifier_free_DDIM.pth')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T08:44:11.790130Z","iopub.execute_input":"2025-05-17T08:44:11.790387Z","iopub.status.idle":"2025-05-17T08:44:11.826992Z","shell.execute_reply.started":"2025-05-17T08:44:11.790367Z","shell.execute_reply":"2025-05-17T08:44:11.826066Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_35/578113755.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'./saved_models/Classifier_free_DDIM.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    941\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    942\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_use_new_zipfile_serialization\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 943\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    944\u001b[0m             _save(\n\u001b[1;32m    945\u001b[0m                 \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_zipfile_writer\u001b[0;34m(name_or_buffer)\u001b[0m\n\u001b[1;32m    808\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m         \u001b[0mcontainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_open_zipfile_writer_buffer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 810\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcontainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    811\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    779\u001b[0m             )\n\u001b[1;32m    780\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 781\u001b[0;31m             \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPyTorchFileWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_compute_crc32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    782\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    783\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Parent directory ./saved_models does not exist."],"ename":"RuntimeError","evalue":"Parent directory ./saved_models does not exist.","output_type":"error"}],"execution_count":26},{"cell_type":"code","source":"# load in model weights and set to eval mode\ncheckpoint = torch.load('/kaggle/input/models/model_epoch_150.pt', map_location=device)\nnn_model.load_state_dict(checkpoint['model_state_dict'])\nnn_model.eval()\nprint(\"Loaded in Model\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ddim_generated_images = gaussian_diffusion.ddim_sample(nn_model, 16, batch_size=32, channels=3, ddim_timesteps=100, n_class=5,\n                                                       w=5.0, mode='random', ddim_discr_method='quad', ddim_eta=0.0, clip_denoised=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# ddim generate new images\nfig = plt.figure(figsize=(16, 16), constrained_layout=True)\ngs = fig.add_gridspec(4, 8)\n\nimgs = ddim_generated_images[:32]\nimgs = imgs.reshape(4, 8, 3, 16, 16)\n\nfor n_row in range(4):\n    for n_col in range(8):\n        f_ax = fig.add_subplot(gs[n_row, n_col])\n        img = imgs[n_row, n_col].transpose(1, 2, 0)  # [3,16,16] → [16,16,3]\n        img = ((img + 1.0) * 127.5).clip(0, 255).astype(np.uint8)  # [-1,1] → [0,255]\n        \n        f_ax.imshow(img, interpolation='nearest')  # 保留像素風格\n        f_ax.axis(\"off\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ngif_generated_images = gaussian_diffusion.ddim_sample(nn_model, 16, batch_size=100, channels=3, ddim_timesteps=100, n_class=5,\n                                                       w=5.0, mode='all', ddim_discr_method='quad', ddim_eta=0.0, clip_denoised=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"fig = plt.figure(figsize=(16, 16), constrained_layout=True)\ngs = fig.add_gridspec(4, 8)\nprint(\"Shape of gif_generated_images[-1]:\", gif_generated_images[-1].shape)\nprint(\"Size of gif_generated_images[-1]:\", gif_generated_images[-1].size)\nimgs = gif_generated_images[-1][:32]\nimgs = imgs.reshape(4, 8, 3, 16, 16)\nfor n_row in range(4):\n    for n_col in range(8):\n        f_ax = fig.add_subplot(gs[n_row, n_col])\n        img = imgs[n_row, n_col].transpose(1, 2, 0)  # [3,16,16] → [16,16,3]\n        img = ((img + 1.0) * 127.5).clip(0, 255).astype(np.uint8)  # [-1,1] → [0,255]\n        \n        f_ax.imshow(img, interpolation='nearest')  # 保留像素風格\n        f_ax.axis(\"off\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib\nmatplotlib.use('Agg')\nimport matplotlib.pyplot as plt\n%matplotlib auto\nimport imageio\nfrom glob import glob\n\n# 生成gif\ndef get_imgs(x_seq, imgs_path, h, w, img_size):\n    if not os.path.exists(imgs_path):\n        os.mkdir(imgs_path)\n    for i in tqdm(range(len(x_seq)), desc='generate gif time step', total=len(x_seq)):\n        fig = plt.figure(figsize=(16, 16), constrained_layout=True)\n        gs = fig.add_gridspec(h, w)\n\n        imgs = x_seq[i].reshape(h, w, img_size, img_size)\n        for n_row in range(h):\n            for n_col in range(w):\n                f_ax = fig.add_subplot(gs[n_row, n_col])\n                f_ax.imshow((imgs[n_row, n_col]+1.0) * 255 / 2, cmap=\"gray\")\n                f_ax.axis(\"off\")\n        plt.savefig('{}/{:04d}.jpg'.format(imgs_path, i), dpi=360)  \n        plt.close()\n\ndef get_imgs_RGB(x_seq, imgs_path, h, w, img_size):\n    for i in range(len(x_seq)):\n        fig = plt.figure(figsize=(w * 2, h * 2), constrained_layout=True)\n        gs = fig.add_gridspec(h, w)\n\n        # 確保資料 shape 是 (N, C, H, W)\n        imgs = x_seq[i]\n        N = imgs.shape[0]\n        assert N >= h * w, f\"圖片數量不足，只有 {N} 張，無法填滿 {h}x{w} grid\"\n\n        imgs = imgs[:h * w]  # 只取需要的張數\n        imgs = imgs.reshape(h, w, *imgs.shape[1:])  # (h, w, C, H, W)\n\n        for n_row in range(h):\n            for n_col in range(w):\n                f_ax = fig.add_subplot(gs[n_row, n_col])\n                img = imgs[n_row, n_col].transpose(1, 2, 0)  # [C, H, W] → [H, W, C]\n                img = ((img + 1.0) * 127.5).clip(0, 255).astype(np.uint8)  # [-1,1] → [0,255]\n                f_ax.imshow(img, interpolation='nearest')\n                f_ax.axis(\"off\")\n\n        plt.savefig(os.path.join(imgs_path, f\"{i:05d}.jpg\"))\n        plt.close()\n\ndef compose_gif(img_paths, output_path, fps=10):\n    print(img_paths[:12])\n    gif_images = []\n    for path in img_paths:\n        gif_images.append(imageio.imread(path))\n    imageio.mimsave(output_path,gif_images,fps=fps)\n\ndef generate_dif(x_seq, img_path, output_path, fps, h, w, img_size, delete_imgs=True):\n    print('start generate images')\n    os.makedirs(img_path, exist_ok=True)\n    get_imgs_RGB(x_seq, img_path, h, w, img_size)\n    print('start generate gif')\n    img_path = img_path + '/*.jpg'\n    img_ls = sorted(glob(img_path))\n    compose_gif(img_ls, output_path, fps)\n    print('start delete images')\n    if delete_imgs:\n        for i in img_ls:\n            os.remove(i)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"generate_dif(gif_generated_images, './gif_generate', './gif_generate/generate_pixelart.gif', 5, 4, 8, 16)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
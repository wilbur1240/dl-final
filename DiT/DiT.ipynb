{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diffusion Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as '_psutil_linux' could not be imported from 'most likely due to a circular import'.\n",
      "\u001b[1;31mClick <a href='https://aka.ms/kernelFailuresModuleImportErrFromFile'>here</a> for more info."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import math\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from itertools import cycle\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.utils as vutils\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.utils.data import DataLoader\n",
    "from accelerate import Accelerator\n",
    "from tqdm.auto import tqdm\n",
    "from einops import rearrange\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, filename, image_size):\n",
    "        self.sprites = np.load(filename)\n",
    "        print(f\"sprite shape: {self.sprites.shape}\")\n",
    "        self.transform = transforms.Compose([\n",
    "            # T.Resize(image_size),\n",
    "            transforms.RandomHorizontalFlip(p=0.5),\n",
    "            transforms.RandomResizedCrop(image_size, scale=(0.9, 1.0)),\n",
    "            transforms.ColorJitter(0.1, 0.1, 0.1, 0.05),\n",
    "            transforms.ToTensor(), # from [0, 255] to range [0.0,1.0]\n",
    "            transforms.Normalize((0.5,), (0.5,)) # range [-1,1]\n",
    "        ])\n",
    "        self.image_size = image_size\n",
    "        self.sprites_shape = self.sprites.shape\n",
    "                \n",
    "    # Return the number of images in the dataset\n",
    "    def __len__(self):\n",
    "        return len(self.sprites)\n",
    "    \n",
    "    # Get the image and label at a given index\n",
    "    def __getitem__(self, idx):\n",
    "        # Return the image and label as a tuple\n",
    "        if self.transform:\n",
    "            image = self.transform(self.sprites[idx])\n",
    "        return (image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Noise Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_beta_schedule(timesteps):\n",
    "    \"\"\"\n",
    "    linear schedule, proposed in original paper\n",
    "    \"\"\"\n",
    "    scale = 1000 / timesteps\n",
    "    beta_start = scale * 0.0001\n",
    "    beta_end = scale * 0.02\n",
    "    return torch.linspace(beta_start, beta_end, timesteps, dtype = torch.float64)\n",
    "\n",
    "# e.g. from Nichol & Dhariwal’s “improved DDPM”:\n",
    "def cosine_beta_schedule(timesteps, s=0.008):\n",
    "    steps = timesteps + 1\n",
    "    x = torch.linspace(0, timesteps, steps)\n",
    "    alphas_cumprod = torch.cos(((x / timesteps) + s) / (1 + s) * math.pi * 0.5) ** 2\n",
    "    alphas_cumprod = alphas_cumprod / alphas_cumprod[0]\n",
    "    betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])\n",
    "    return betas.clamp(max=0.999)\n",
    "\n",
    "def extract(a, t, x_shape):\n",
    "    b, *_ = t.shape\n",
    "    out = a.gather(-1, t)\n",
    "    return out.reshape(b, *((1,) * (len(x_shape) - 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## network modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, in_channels=3, patch_size=8, emb_dim=512, img_size=64):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.emb_dim = emb_dim\n",
    "        self.img_size = img_size\n",
    "        self.proj = nn.Conv2d(in_channels, emb_dim, kernel_size=patch_size, stride=patch_size)\n",
    "        self.pos_embedding = nn.Parameter(torch.randn((img_size // patch_size)**2, emb_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x)  # [B, emb_dim, H/patch, W/patch]\n",
    "        x = rearrange(x, 'b c h w -> b (h w) c')\n",
    "        return x + self.pos_embedding\n",
    "\n",
    "class SinusoidalPosEmb(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, t):\n",
    "        device = t.device\n",
    "        half_dim = self.dim // 2\n",
    "        emb = torch.exp(\n",
    "            torch.arange(half_dim, device=device) * -(math.log(10000) / (half_dim - 1))\n",
    "        )\n",
    "        emb = t[:, None] * emb[None, :]\n",
    "        emb = torch.cat((emb.sin(), emb.cos()), dim=-1)\n",
    "        return emb  # [B, dim]\n",
    "\n",
    "class TransformerBackbone(nn.Module):\n",
    "    def __init__(self, in_channels=3, img_size=64, patch_size=8, emb_dim=512, depth=6, num_heads=8, time_emb_dim=128):\n",
    "        super().__init__()\n",
    "        self.embed = PatchEmbedding(in_channels, patch_size, emb_dim, img_size)\n",
    "\n",
    "        self.time_embedding = SinusoidalPosEmb(time_emb_dim)\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            nn.Linear(time_emb_dim, emb_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(emb_dim, emb_dim)\n",
    "        )\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=emb_dim, nhead=num_heads, dim_feedforward=1024, batch_first=True)\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=depth)\n",
    "        self.output_proj = nn.Sequential(\n",
    "            nn.Linear(emb_dim, patch_size * patch_size * in_channels),\n",
    "        )\n",
    "        self.patch_size = patch_size\n",
    "        self.img_size = img_size\n",
    "        self.channels = in_channels\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        B = x.size(0)\n",
    "        patches = self.embed(x)  # [B, N, D]\n",
    "\n",
    "        t_emb = self.time_embedding(t) # [B, time_emb_dim]\n",
    "        t_emb = self.time_mlp(t_emb) # [B, D]\n",
    "        t_emb = t_emb.unsqueeze(1) # [B, 1, D]\n",
    "\n",
    "        patches = patches + t_emb # Broadcast time embedding to all patches\n",
    "        \n",
    "        encoded = self.encoder(patches)  # [B, N, D]\n",
    "        decoded = self.output_proj(encoded)  # [B, N, patch*patch*channels]\n",
    "        decoded = decoded.view(B, -1, self.channels, self.patch_size, self.patch_size)  # reshape into patches\n",
    "        h = w = self.img_size // self.patch_size\n",
    "        decoded = rearrange(decoded, 'b (h w) c ph pw -> b c (h ph) (w pw)', h=h, w=w)\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diffusion process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalization functions\n",
    "def normalize_to_neg_one_to_one(img):\n",
    "    return img * 2 - 1\n",
    "\n",
    "def unnormalize_to_zero_to_one(t):\n",
    "    return (t + 1) * 0.5\n",
    "\n",
    "class GaussianDiffusion(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: nn.Module,\n",
    "        image_size: int, \n",
    "        timesteps: int,\n",
    "        beta_schedule: str = 'linear',\n",
    "        auto_normalize: bool = True,\n",
    "        lambda_l1: float = 0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = model\n",
    "        self.channels = model.channels\n",
    "        self.image_size = image_size\n",
    "        self.num_timesteps = timesteps\n",
    "        self.lambda_l1 = lambda_l1\n",
    "\n",
    "        # 1) build beta schedule\n",
    "        if beta_schedule == 'linear':\n",
    "            betas = linear_beta_schedule(timesteps)\n",
    "        elif beta_schedule == 'cosine':\n",
    "            betas = cosine_beta_schedule(timesteps)\n",
    "        else:\n",
    "            raise ValueError(f'Unknown beta schedule: {beta_schedule}')\n",
    "        \n",
    "        alphas = 1.0 - betas\n",
    "        alphas_cumprod = torch.cumprod(alphas, dim=0)\n",
    "        alphas_cumprod_prev = F.pad(alphas_cumprod[:-1], (1, 0), value=1.0)\n",
    "\n",
    "        # 2) register all buffers\n",
    "        self.register_buffer('betas', betas.float())\n",
    "        self.register_buffer('alphas_cumprod', alphas_cumprod.float())\n",
    "        self.register_buffer('alphas_cumprod_prev', alphas_cumprod_prev.float())\n",
    "        self.register_buffer('sqrt_alphas_cumprod', torch.sqrt(alphas_cumprod).float())\n",
    "        self.register_buffer('sqrt_one_minus_alphas_cumprod', torch.sqrt(1 - alphas_cumprod).float())\n",
    "        self.register_buffer('sqrt_recip_alphas_cumprod', torch.sqrt(1.0 / alphas_cumprod).float())\n",
    "        self.register_buffer('sqrt_recipm1_alphas_cumprod', torch.sqrt(1.0 / alphas_cumprod - 1.0).float())\n",
    "\n",
    "        # posterior q(x_{t-1} | x_t, x_0)\n",
    "        posterior_variance = betas * (1 - alphas_cumprod_prev) / (1 - alphas_cumprod)\n",
    "        self.register_buffer('posterior_variance', posterior_variance.float())\n",
    "        self.register_buffer('posterior_log_variance_clipped', torch.log(posterior_variance.clamp(min=1e-20)).float())\n",
    "        self.register_buffer('posterior_mean_coef1', (betas * torch.sqrt(alphas_cumprod_prev) / (1 - alphas_cumprod)).float())\n",
    "        self.register_buffer('posterior_mean_coef2', ((1 - alphas_cumprod_prev) * torch.sqrt(alphas) / (1 - alphas_cumprod)).float())\n",
    "\n",
    "        # 3) Normalization functions\n",
    "        self.normalize = normalize_to_neg_one_to_one if auto_normalize else (lambda x: x)\n",
    "        self.unnormalize = unnormalize_to_zero_to_one if auto_normalize else (lambda x: x)\n",
    "\n",
    "    def dynamic_threshold(self, x0, percentile=0.995):\n",
    "        \"\"\" Clips x0 dynamically to avoid over/under-exposed outputs \"\"\"\n",
    "        s = torch.quantile(x0.abs().flatten(1), percentile, dim=1)\n",
    "        s = torch.maximum(s, torch.ones_like(s))[:, None, None, None]\n",
    "        return x0.clamp(-s, s) / s\n",
    "\n",
    "    def predict_start_from_noise(self, x_t, t, noise):\n",
    "        \"\"\"Estimate x_0 from x_t and predicted noise\"\"\"\n",
    "        return (\n",
    "            extract(self.sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t \n",
    "            - extract(self.sqrt_recipm1_alphas_cumprod, t, x_t.shape) * noise\n",
    "        )\n",
    "    \n",
    "    def q_posterior(self, x_start, x_t, t):\n",
    "        \"\"\" Compute mean & variance of q(x_{t-1}) | x_t, x_0 \"\"\"\n",
    "        mean = (\n",
    "            extract(self.posterior_mean_coef1, t, x_t.shape) * x_start +\n",
    "            extract(self.posterior_mean_coef2, t, x_t.shape) * x_t\n",
    "        )\n",
    "        var = extract(self.posterior_variance, t, x_t.shape)\n",
    "        log_var = extract(self.posterior_log_variance_clipped, t, x_t.shape)\n",
    "        return mean, var, log_var\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def p_sample(self, x, t: int, clip_denoised: bool = True):\n",
    "        \"\"\" One diffusion reverse step \"\"\"\n",
    "        b = x.shape[0]\n",
    "        t_batch = torch.full((b,), t, device=x.device, dtype=torch.long)\n",
    "        # model predicts noise\n",
    "        pred_noise = self.model(x, t_batch)\n",
    "        # recover x0; optionally clamp it\n",
    "        x0 = self.predict_start_from_noise(x, t_batch, pred_noise)\n",
    "        if clip_denoised:\n",
    "            x0 = self.dynamic_threshold(x0)\n",
    "\n",
    "        # posterior mean & variance\n",
    "        mean, _, log_var = self.q_posterior(x0, x, t_batch)\n",
    "        noise = torch.randn_like(x) if t > 0 else 0.0\n",
    "        return mean + torch.exp(0.5 * log_var) * noise\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def p_sample_loop(self, shape, return_all_timesteps: bool = False):\n",
    "        \"\"\" Starting from pure noise, run full reverse chain\"\"\"\n",
    "        img = torch.randn(shape, device=self.betas.device)\n",
    "        all_imgs = [img]\n",
    "        for t in tqdm(reversed(range(self.num_timesteps)), desc='sampling'):\n",
    "            img = self.p_sample(img, t)\n",
    "            all_imgs.append(img)\n",
    "        out = torch.stack(all_imgs, dim=1) if return_all_timesteps else img\n",
    "        return self.unnormalize(out)\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def p_sample_ddim(self, x, t, t_prev, eta=0.0, clip_denoised=True):\n",
    "        \"\"\" One DDIM sampling step from x_t to x_{t_prev}\"\"\"\n",
    "        b = x.shape[0]\n",
    "        t_batch = torch.full((b, ), t, device=x.device, dtype=torch.long)\n",
    "        # predict noise\n",
    "        pred_noise = self.model(x, t_batch)\n",
    "        # pred x0\n",
    "        x0 = self.predict_start_from_noise(x, t_batch, pred_noise)\n",
    "        if clip_denoised:\n",
    "            x0 = self.dynamic_threshold(x0)\n",
    "\n",
    "        alpha_t = self.alphas_cumprod[t]\n",
    "        alpha_prev = self.alphas_cumprod[t_prev] if t_prev >= 0 else torch.tensor(1.0, device=x.device)\n",
    "        sqrt_alpha_t = alpha_t.sqrt()\n",
    "        sqrt_alpha_prev = alpha_prev.sqrt()\n",
    "        sigma_t = eta * ((1 - alpha_prev) / (1 - alpha_t) * (1 - alpha_t / alpha_prev)).sqrt()\n",
    "        pred_dir = (1 - alpha_prev - sigma_t ** 2).sqrt() * pred_noise\n",
    "        noise = sigma_t * torch.randn_like(x) if t_prev >= 0 else 0.0\n",
    "        x_prev = sqrt_alpha_prev * x0 + pred_dir + noise\n",
    "\n",
    "        return x_prev, x0\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def ddim_sample_loop(self, shape, num_ddim_steps=50, eta=0.0, return_all_timesteps=False):\n",
    "        \"\"\" Run the fuull DDIM sampling loop \"\"\"\n",
    "        device = self.betas.device\n",
    "        img = torch.randn(shape, device=device)\n",
    "        all_imgs = [img]\n",
    "\n",
    "        # Create a custom timestep schedule\n",
    "        ddim_timesteps = np.linspace(0, self.num_timesteps - 1, num_ddim_steps, dtype=int)\n",
    "\n",
    "        for i in tqdm(range(num_ddim_steps - 1, -1, -1), desc='DDIM sampling'):\n",
    "            t = ddim_timesteps[i]\n",
    "            t_prev = ddim_timesteps[i - 1] if i > 0 else -1\n",
    "\n",
    "            img, _ = self.p_sample_ddim(img, t, t_prev, eta=eta)\n",
    "            all_imgs.append(img)\n",
    "        \n",
    "        out = torch.stack(all_imgs, dim=1) if return_all_timesteps else img\n",
    "        return self.unnormalize(out)\n",
    "\n",
    "    def sample(self, batch_size=16, use_ddim=False, num_ddim_steps=50, eta=0.0, return_all_timesteps=False):\n",
    "        shape = (batch_size, self.channels, self.image_size, self.image_size)\n",
    "        if use_ddim:\n",
    "            return self.ddim_sample_loop(shape, num_ddim_steps=num_ddim_steps, eta=eta, return_all_timesteps=return_all_timesteps)\n",
    "        else:\n",
    "            return self.p_sample_loop(shape, return_all_timesteps=return_all_timesteps)\n",
    "    \n",
    "    def q_sample(self, x_start, t, noise=None):\n",
    "        \"\"\" Forward noising process \"\"\"\n",
    "        if noise is None:\n",
    "            noise = torch.randn_like(x_start)\n",
    "        return (\n",
    "            extract(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start + \n",
    "            extract(self.sqrt_one_minus_alphas_cumprod, t, x_start.shape) * noise\n",
    "        )\n",
    "    \n",
    "    def p_losses(self, x_start, t, noise=None):\n",
    "        \"\"\" MSE + L1 loss between true noise and model's prediction \"\"\"\n",
    "        if noise is None:\n",
    "            noise = torch.randn_like(x_start)\n",
    "        x_noisy = self.q_sample(x_start, t, noise)\n",
    "        pred_noise = self.model(x_noisy, t)\n",
    "        mse_loss = F.mse_loss(pred_noise, noise, reduction='none')\n",
    "        mse_loss = mse_loss.mean(dim=list(range(1, mse_loss.ndim))) # mean over c, h, w\n",
    "\n",
    "        # x_recon = self.predict_start_from_noise(x_noisy, t, pred_noise)\n",
    "        # x_recon = x_recon.clamp(-1., -1.)\n",
    "        # l1_loss = F.l1_loss(x_recon, x_start, reduction='none')\n",
    "        # l1_loss = l1_loss.mean(dim=list(range(1, l1_loss.ndim)))\n",
    "\n",
    "        # loss = mse_loss + self.lambda_l1 * l1_loss\n",
    "        loss = mse_loss\n",
    "        return loss.mean()\n",
    "    \n",
    "    def forward(self, img):\n",
    "        \"\"\" Training entrypoint: sample random timesteps & return loss \"\"\"\n",
    "        b, c, h, w = img.shape\n",
    "        assert h == self.image_size and w == self.image_size\n",
    "        t = torch.randint(0, self.num_timesteps, (b, ), device=img.device)\n",
    "        img = self.normalize(img)\n",
    "        return self.p_losses(img, t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        diffusion_model: nn.Module,\n",
    "        data_folder: str,\n",
    "        batch_size: int = 16,\n",
    "        lr: float = 1e-4,\n",
    "        num_steps: int = 100000,\n",
    "        grad_accum_steps: int = 1,\n",
    "        ema_decay: float = 0.995,\n",
    "        save_interval: int = 1000,\n",
    "        num_samples: int = 25,\n",
    "        results_folder: str = './results',\n",
    "        use_ddim = False,\n",
    "        num_ddim_steps=50,\n",
    "        eta = 0.0\n",
    "    ):\n",
    "        # Accelerator\n",
    "        self.accelerator = Accelerator(mixed_precision='fp16')\n",
    "        self.device = self.accelerator.device\n",
    "\n",
    "        # Training State\n",
    "        self.batch_size       = batch_size\n",
    "        self.grad_accum_steps = grad_accum_steps\n",
    "        self.num_steps        = num_steps\n",
    "        self.save_interval    = save_interval\n",
    "        self.num_samples      = num_samples\n",
    "\n",
    "        # model, optimizer, EMA\n",
    "        self.model = diffusion_model.to(self.device)\n",
    "        self.optimizer = Adam(self.model.parameters(), lr=lr)\n",
    "        self.scheduler = CosineAnnealingLR(self.optimizer, T_max=self.num_steps, eta_min=1e-6)\n",
    "        self.model, self.optimizer = self.accelerator.prepare(\n",
    "            self.model, self.optimizer\n",
    "        )\n",
    "        \n",
    "        # Use EMA on the raw model\n",
    "        self.ema = EMA(self.accelerator.unwrap_model(self.model), beta=ema_decay)\n",
    "\n",
    "        # Data\n",
    "        ds = Dataset(data_folder, diffusion_model.image_size)\n",
    "        dl = DataLoader(ds, batch_size=batch_size, shuffle=True, num_workers=os.cpu_count())\n",
    "        self.dl = cycle(self.accelerator.prepare(dl))\n",
    "\n",
    "        # checkpoints & samples\n",
    "        self.results_folder = Path(results_folder)\n",
    "        self.results_folder.mkdir(parents=True, exist_ok=True)\n",
    "        self.step = 0\n",
    "        self.use_ddim = use_ddim\n",
    "        self.num_ddim_steps = num_ddim_steps\n",
    "        self.eta = eta\n",
    "    \n",
    "    def save(self, milestone: int):\n",
    "        \"\"\"Save model, optimizer, EMA and step counter.\"\"\"\n",
    "        if not self.accelerator.is_main_process:\n",
    "            return\n",
    "        ckpt = self.results_folder / f'model-{milestone}.pt'\n",
    "        data = {\n",
    "            'step': self.step,\n",
    "            'model': self.accelerator.get_state_dict(self.model),\n",
    "            'optimizer': self.optimizer.state_dict(),\n",
    "            'ema':       self.ema.state_dict(),\n",
    "        }\n",
    "        torch.save(data, ckpt)\n",
    "\n",
    "    def load(self, ckpt_path: str):\n",
    "        \"\"\"Load all state (model, optimizer, EMA, step).\"\"\"\n",
    "        data = torch.load(ckpt_path, map_location=self.device)\n",
    "        raw_model = self.accelerator.unwrap_model(self.model)\n",
    "        raw_model.load_state_dict(data['model'])\n",
    "        self.optimizer.load_state_dict(data['optimizer'])\n",
    "        self.ema.load_state_dict(data['ema'])\n",
    "        self.step = data['step']\n",
    "\n",
    "    def _sample_and_save(self, milestone: int):\n",
    "        \"\"\"Generate `num_samples` via EMA model and save grid.\"\"\"\n",
    "        self.ema.ema_model.eval()\n",
    "        batches = num_to_groups(self.num_samples, self.batch_size)\n",
    "        imgs = torch.cat([\n",
    "            self.ema.ema_model.sample(batch_size=n, use_ddim=self.use_ddim, num_ddim_steps=self.num_ddim_steps, eta=self.eta) for n in batches\n",
    "        ], dim=0)\n",
    "        path = self.results_folder / f'sample-{milestone}.png'\n",
    "        vutils.save_image(imgs, path, nrow=int(math.sqrt(self.num_samples)))\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"Run the training loop with gradient accumulation, EMA updates, and periodic sampling.\"\"\"\n",
    "        pbar = tqdm(total=self.num_steps, initial=self.step, disable=not self.accelerator.is_main_process)\n",
    "        while self.step < self.num_steps:\n",
    "            total_loss = 0.0\n",
    "\n",
    "            # gradient accumulation\n",
    "            for _ in range(self.grad_accum_steps):\n",
    "                batch = next(self.dl).to(self.device)\n",
    "                with self.accelerator.autocast():\n",
    "                    loss = self.model(batch) / self.grad_accum_steps\n",
    "                total_loss += loss.item()\n",
    "                self.accelerator.backward(loss)\n",
    "\n",
    "            # optimizer step\n",
    "            self.accelerator.clip_grad_norm_(self.model.parameters(), 1.0)\n",
    "            self.optimizer.step()\n",
    "            self.scheduler.step()\n",
    "            self.optimizer.zero_grad()\n",
    "            self.step += 1\n",
    "\n",
    "            # EMA & sampling\n",
    "            if self.accelerator.is_main_process:\n",
    "                self.ema.update()\n",
    "                if self.step % self.save_interval == 0:\n",
    "                    milestone = self.step // self.save_interval\n",
    "                    self._sample_and_save(milestone)\n",
    "                    self.save(milestone)\n",
    "\n",
    "            pbar.set_description(f'loss: {total_loss:.4f}')\n",
    "            pbar.update(1)\n",
    "\n",
    "        if self.accelerator.is_main_process:\n",
    "            print('Training complete.')\n",
    "\n",
    "    def inference(self, total: int = 1000, output_path: str = './submission'):\n",
    "        \"\"\"\n",
    "        Generate `total` images using the same batch size as training\n",
    "        (self.batch_size, e.g. 16), and save them to disk.\n",
    "        \"\"\"\n",
    "        Path(output_path).mkdir(parents=True, exist_ok=True)\n",
    "        count = 0\n",
    "        batch_size = self.batch_size  # manually fixed at 16\n",
    "\n",
    "        with torch.no_grad():\n",
    "            while count < total:\n",
    "                n = min(batch_size, total - count)\n",
    "\n",
    "                # clear any leftover GPU memory\n",
    "                if torch.cuda.is_available():\n",
    "                    gc.collect()\n",
    "                    torch.cuda.empty_cache()\n",
    "\n",
    "                # sample n images\n",
    "                imgs = self.ema.ema_model.sample(batch_size=n, use_ddim=self.use_ddim, num_ddim_steps=self.num_ddim_steps, eta=self.eta)\n",
    "\n",
    "                # move to CPU and save\n",
    "                imgs = imgs.cpu()\n",
    "                for img in imgs:\n",
    "                    count += 1\n",
    "                    vutils.save_image(img, f\"{output_path}/{count}.jpg\")\n",
    "\n",
    "                # clean up \n",
    "                del imgs\n",
    "                if torch.cuda.is_available():\n",
    "                    gc.collect()\n",
    "                    torch.cuda.empty_cache()\n",
    "\n",
    "        print(\"Inference complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TransformerBackbone(\n",
    "    in_channels=3, img_size=64, patch_size=8,\n",
    "    emb_dim=512, depth=6, num_heads=8, time_emb_dim=128\n",
    ")\n",
    "diffusion = GaussianDiffusion(\n",
    "    model=model, image_size=64, timesteps=1000,\n",
    "    beta_schedule='cosine', auto_normalize=True\n",
    ")\n",
    "trainer = Trainer(\n",
    "    diffusion_model=diffusion,\n",
    "    data_folder = './',\n",
    "    batch_size=16, lr=1e-4, num_steps=70000,\n",
    "    grad_accum_steps=1, ema_decay=0.995,\n",
    "    save_interval=1000, num_samples=25,\n",
    "    results_folder='./results',\n",
    "    use_ddim=True, num_ddim_steps=50, eta=0.0\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt = './results_56/model-20.pt'\n",
    "trainer.load(ckpt)\n",
    "trainer.inference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diffusion = trainer.model\n",
    "with torch.no_grad():\n",
    "    all_steps = diffusion.sample(batch_size=1, return_all_timesteps=True)\n",
    "# shape [1, T+1, C, H, W]\n",
    "\n",
    "imgs = all_steps.squeeze(0).permute(0, 2, 3, 1).cpu().numpy()\n",
    "imgs = unnormalize_to_zero_to_one(imgs)\n",
    "\n",
    "timesteps = [0, diffusion.num_timesteps // 4, diffusion.num_timesteps // 2,\n",
    "             3 * diffusion.num_timesteps // 4, diffusion.num_timesteps]\n",
    "sel = [imgs[t] for t in timesteps]\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(1, 5, figsize=(15, 5))\n",
    "for ax, im, step in zip(axes, sel, timesteps):\n",
    "    ax.imshow(im)\n",
    "    ax.axis('off')\n",
    "    ax.set_title(f\"Step {step}\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

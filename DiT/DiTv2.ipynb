{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a55eada2",
   "metadata": {},
   "source": [
    "# Diffusion Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993a518e",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2fbaa537",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import math\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.utils as vutils\n",
    "from torchvision import transforms\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from accelerate import Accelerator\n",
    "from tqdm.auto import tqdm\n",
    "from einops import rearrange\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from ema_pytorch import EMA\n",
    "from PIL import Image\n",
    "from torchvision.transforms import ToPILImage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbf0b69",
   "metadata": {},
   "source": [
    "## Data preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5702cf1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, images_npy: str, labels_npy: str, image_size: int):\n",
    "        # load images and labels\n",
    "        self.sprites = np.load(images_npy)     # shape: [N, H, W, C]\n",
    "        raw_labels   = np.load(labels_npy)     # shape: [N, 5] (one-hot) or [N] (ints)\n",
    "\n",
    "        # if one-hot, convert to class indices:\n",
    "        if raw_labels.ndim == 2:\n",
    "            self.labels = raw_labels.argmax(axis=1)\n",
    "        else:\n",
    "            self.labels = raw_labels\n",
    "\n",
    "        self.image_size = image_size\n",
    "        self.transform = transforms.Compose([\n",
    "            ToPILImage(),\n",
    "            transforms.RandomHorizontalFlip(0.5),\n",
    "            transforms.RandomResizedCrop(image_size, scale=(0.9,1.0)),\n",
    "            transforms.ColorJitter(0.1,0.1,0.1,0.05),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5,)*3, (0.5,)*3),\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sprites)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_np = self.sprites[idx]                # H×W×C\n",
    "        img = self.transform(img_np)              # [3, H, W]\n",
    "        label = int(self.labels[idx])             # scalar in {0,…,4}\n",
    "        return img, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c99b01",
   "metadata": {},
   "source": [
    "## Diffusion process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38de3339",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper: extract time-indexed values from buffer\n",
    "def extract(a, t, x_shape):\n",
    "    batch_size = t.shape[0]\n",
    "    out = a.gather(-1, t).float()\n",
    "    return out.reshape(batch_size, *((1,) * (len(x_shape) - 1)))\n",
    "\n",
    "# Normalization functions\n",
    "def normalize_to_neg_one_to_one(img):\n",
    "    return img * 2 - 1\n",
    "\n",
    "def unnormalize_to_zero_to_one(t):\n",
    "    return (t + 1) * 0.5\n",
    "\n",
    "class DiTDiffusion(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: nn.Module,\n",
    "        image_size: int, \n",
    "        timesteps: int,\n",
    "        beta_schedule: str = 'linear',\n",
    "        auto_normalize: bool = True,\n",
    "        lambda_l1: float = 0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.channels = model.channels\n",
    "        self.image_size = image_size\n",
    "        self.num_timesteps = timesteps\n",
    "        self.lambda_l1 = lambda_l1\n",
    "        self.num_classes = model.num_classes\n",
    "\n",
    "        if beta_schedule == 'linear':\n",
    "            betas = torch.linspace(1e-4, 0.02, timesteps)\n",
    "        elif beta_schedule == 'cosine':\n",
    "            steps = timesteps + 1\n",
    "            x = torch.linspace(0, timesteps, steps)\n",
    "            alphas_cumprod = torch.cos(((x / timesteps) + 0.008) / 1.008 * torch.pi * 0.5) ** 2\n",
    "            alphas_cumprod = alphas_cumprod / alphas_cumprod[0]\n",
    "            betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])\n",
    "        else:\n",
    "            raise ValueError(f'Unknown beta schedule: {beta_schedule}')\n",
    "\n",
    "        alphas = 1.0 - betas\n",
    "        alphas_cumprod = torch.cumprod(alphas, dim=0)\n",
    "        alphas_cumprod_prev = F.pad(alphas_cumprod[:-1], (1, 0), value=1.0)\n",
    "\n",
    "        self.register_buffer('betas', betas.float())\n",
    "        self.register_buffer('alphas_cumprod', alphas_cumprod.float())\n",
    "        self.register_buffer('alphas_cumprod_prev', alphas_cumprod_prev.float())\n",
    "        self.register_buffer('sqrt_alphas_cumprod', torch.sqrt(alphas_cumprod).float())\n",
    "        self.register_buffer('sqrt_one_minus_alphas_cumprod', torch.sqrt(1 - alphas_cumprod).float())\n",
    "        self.register_buffer('sqrt_recip_alphas_cumprod', torch.sqrt(1.0 / alphas_cumprod).float())\n",
    "        self.register_buffer('sqrt_recipm1_alphas_cumprod', torch.sqrt(1.0 / alphas_cumprod - 1.0).float())\n",
    "        self.register_buffer('posterior_variance', betas * (1 - alphas_cumprod_prev) / (1 - alphas_cumprod))\n",
    "        self.register_buffer('posterior_log_variance_clipped', torch.log(self.posterior_variance.clamp(min=1e-20)).float())\n",
    "        self.register_buffer('posterior_mean_coef1', (betas * torch.sqrt(alphas_cumprod_prev) / (1 - alphas_cumprod)).float())\n",
    "        self.register_buffer('posterior_mean_coef2', ((1 - alphas_cumprod_prev) * torch.sqrt(alphas) / (1 - alphas_cumprod)).float())\n",
    "\n",
    "        self.normalize = normalize_to_neg_one_to_one if auto_normalize else (lambda x: x)\n",
    "        self.unnormalize = unnormalize_to_zero_to_one if auto_normalize else (lambda x: x)\n",
    "\n",
    "    def predict_start_from_noise(self, x_t, t, noise):\n",
    "        return (\n",
    "            extract(self.sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t -\n",
    "            extract(self.sqrt_recipm1_alphas_cumprod, t, x_t.shape) * noise\n",
    "        )\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def p_sample(self, x, t: int, clip_denoised: bool = True):\n",
    "        \"\"\" One diffusion reverse step \"\"\"\n",
    "        b = x.shape[0]\n",
    "        t_batch = torch.full((b,), t, device=x.device, dtype=torch.long)\n",
    "        # model predicts noise\n",
    "        pred_noise = self.model(x, t_batch)\n",
    "        # recover x0; optionally clamp it\n",
    "        x0 = self.predict_start_from_noise(x, t_batch, pred_noise)\n",
    "        x0 = self.dynamic_threshold(x0)\n",
    "\n",
    "        # posterior mean & variance\n",
    "        mean, _, log_var = self.q_posterior(x0, x, t_batch)\n",
    "        noise = torch.randn_like(x) if t > 0 else 0.0\n",
    "        return mean + torch.exp(0.5 * log_var) * noise\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def p_sample_loop(self, shape, return_all_timesteps: bool = False):\n",
    "        img = torch.randn(shape, device=self.betas.device)\n",
    "        if return_all_timesteps:\n",
    "            all_imgs = [img]\n",
    "            for t in tqdm(reversed(range(self.num_timesteps)), desc='sampling'):\n",
    "                img = self.p_sample(img, t)\n",
    "                all_imgs.append(img)\n",
    "            out = torch.stack(all_imgs, dim=1)\n",
    "        else:\n",
    "            for t in tqdm(reversed(range(self.num_timesteps)), desc='sampling'):\n",
    "                img = self.p_sample(img, t)\n",
    "            out = img\n",
    "        return self.unnormalize(out)\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def p_sample_cfg(self, x, t, labels, guidance_scale):\n",
    "        \"\"\"\n",
    "        x:       [B, C, H, W]\n",
    "        t:       integer timestep\n",
    "        labels:  LongTensor [B] with the class IDs\n",
    "        \"\"\"\n",
    "        b = x.shape[0]\n",
    "        device = x.device\n",
    "\n",
    "        # 1) duplicate inputs for unconditional + conditional\n",
    "        x_in = torch.cat([x, x], dim=0)\n",
    "        t_in = torch.full((2*b,), t, device=device, dtype=torch.long)\n",
    "        # first half: uncond (we pass dummy label, e.g. zeros)\n",
    "        # second half: real labels\n",
    "        lbl_uncond = torch.full((b,), self.num_classes, device=device, dtype=torch.long)\n",
    "        lbl_cond   = labels\n",
    "        labels_in  = torch.cat([lbl_uncond, lbl_cond], dim=0)\n",
    "\n",
    "        # 2) predict noise for both branches\n",
    "        eps_all = self.model(x_in, t_in, labels_in)  # model must accept labels!\n",
    "        eps_uncond, eps_cond = eps_all.chunk(2, dim=0)\n",
    "\n",
    "        # 3) fuse via CFG\n",
    "        eps = eps_uncond + guidance_scale * (eps_cond - eps_uncond)\n",
    "\n",
    "        # 4) standard posterior\n",
    "        x0 = self.predict_start_from_noise(x, t_in[:b], eps)\n",
    "        x0 = self.dynamic_threshold(x0)\n",
    "        mean, _, log_var = self.q_posterior(x0, x, t_in[:b])\n",
    "        noise = torch.randn_like(x) if t > 0 else 0.0\n",
    "        return mean + torch.exp(0.5 * log_var) * noise\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def p_sample_cfg_loop(self, shape, labels, guidance_scale=0.0):\n",
    "        img = torch.randn(shape, device=self.betas.device)\n",
    "        for t in reversed(range(self.num_timesteps)):\n",
    "            if guidance_scale > 0:\n",
    "                img = self.p_sample_cfg(img, t, labels, guidance_scale)\n",
    "            else:\n",
    "                img = self.p_sample(img, t)\n",
    "        return self.unnormalize(img)\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def p_sample_ddim(self, x, t, t_prev, eta=0.0, clip_denoised=True):\n",
    "        \"\"\" One DDIM sampling step from x_t to x_{t_prev}\"\"\"\n",
    "        b = x.shape[0]\n",
    "        t_batch = torch.full((b, ), t, device=x.device, dtype=torch.long)\n",
    "        # predict noise\n",
    "        pred_noise = self.model(x, t_batch)\n",
    "        # pred x0\n",
    "        x0 = self.predict_start_from_noise(x, t_batch, pred_noise)\n",
    "        x0 = self.dynamic_threshold(x0)\n",
    "\n",
    "        alpha_t = self.alphas_cumprod[t]\n",
    "        alpha_prev = self.alphas_cumprod[t_prev] if t_prev >= 0 else torch.tensor(1.0, device=x.device)\n",
    "        sqrt_alpha_t = alpha_t.sqrt()\n",
    "        sqrt_alpha_prev = alpha_prev.sqrt()\n",
    "        sigma_t = eta * ((1 - alpha_prev) / (1 - alpha_t) * (1 - alpha_t / alpha_prev)).sqrt()\n",
    "        pred_dir = (1 - alpha_prev - sigma_t ** 2).sqrt() * pred_noise\n",
    "        noise = sigma_t * torch.randn_like(x) if t_prev >= 0 else 0.0\n",
    "        x_prev = sqrt_alpha_prev * x0 + pred_dir + noise\n",
    "\n",
    "        return x_prev, x0\n",
    "    \n",
    "\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def ddim_sample_loop(self, shape, num_ddim_steps=50, eta=0.0, return_all_timesteps=False):\n",
    "        \"\"\" Run the full DDIM sampling loop \"\"\"\n",
    "        device = self.betas.device\n",
    "        img = torch.randn(shape, device=device)\n",
    "        all_imgs = [img]\n",
    "\n",
    "        # Create a custom timestep schedule\n",
    "        ddim_timesteps = np.linspace(0, self.num_timesteps - 1, num_ddim_steps, dtype=int)\n",
    "\n",
    "        for i in tqdm(range(num_ddim_steps - 1, -1, -1), desc='DDIM sampling'):\n",
    "            t = ddim_timesteps[i]\n",
    "            t_prev = ddim_timesteps[i - 1] if i > 0 else -1\n",
    "\n",
    "            img, _ = self.p_sample_ddim(img, t, t_prev, eta=eta)\n",
    "            all_imgs.append(img)\n",
    "        \n",
    "        out = torch.stack(all_imgs, dim=1) if return_all_timesteps else img\n",
    "        return self.unnormalize(out)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sample(self, batch_size=16, use_ddim=False, use_cfg=False, num_ddim_steps=50, eta=0.0, return_all_timesteps=False, labels=None, guidance_scale=0.0):\n",
    "        shape = (batch_size, self.channels, self.image_size, self.image_size)\n",
    "\n",
    "        if use_ddim:\n",
    "            return self.ddim_sample_loop(\n",
    "                shape,\n",
    "                num_ddim_steps=num_ddim_steps,\n",
    "                eta=eta,\n",
    "                return_all_timesteps=return_all_timesteps\n",
    "            )\n",
    "        elif use_cfg:\n",
    "            if labels is None:\n",
    "                labels = torch.zeros(batch_size, dtype=torch.long, device=self.betas.device)\n",
    "            return self.p_sample_cfg_loop(shape, labels, guidance_scale)\n",
    "        else:\n",
    "            return self.p_sample_loop(shape, return_all_timesteps=return_all_timesteps)\n",
    "\n",
    "    def q_sample(self, x_start, t, noise=None):\n",
    "        if noise is None:\n",
    "            noise = torch.randn_like(x_start)\n",
    "        return (\n",
    "            extract(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start +\n",
    "            extract(self.sqrt_one_minus_alphas_cumprod, t, x_start.shape) * noise\n",
    "        )\n",
    "\n",
    "    def q_posterior(self, x_start, x_t, t):\n",
    "        mean = (\n",
    "            extract(self.posterior_mean_coef1, t, x_t.shape) * x_start +\n",
    "            extract(self.posterior_mean_coef2, t, x_t.shape) * x_t\n",
    "        )\n",
    "        var = extract(self.posterior_variance, t, x_t.shape)\n",
    "        log_var = extract(self.posterior_log_variance_clipped, t, x_t.shape)\n",
    "        return mean, var, log_var\n",
    "\n",
    "    def p_losses(self, x_start, t, noise=None):\n",
    "        \"\"\" MSE + L1 loss between true noise and model's prediction \"\"\"\n",
    "        if noise is None:\n",
    "            noise = torch.randn_like(x_start)\n",
    "        x_noisy = self.q_sample(x_start, t, noise)\n",
    "        pred_noise = self.model(x_noisy, t)\n",
    "        mse_loss = F.mse_loss(pred_noise, noise, reduction='none')\n",
    "        mse_loss = mse_loss.mean(dim=list(range(1, mse_loss.ndim))) # mean over c, h, w\n",
    "\n",
    "        x_recon = self.predict_start_from_noise(x_noisy, t, pred_noise)\n",
    "        x_recon = x_recon.clamp(-1., 1.)\n",
    "        l1_loss = F.l1_loss(x_recon, x_start, reduction='none')\n",
    "        l1_loss = l1_loss.mean(dim=list(range(1, l1_loss.ndim)))\n",
    "\n",
    "        loss = mse_loss + self.lambda_l1 * l1_loss\n",
    "        # loss = mse_loss\n",
    "        return loss.mean()\n",
    "    \n",
    "    def forward(self, img, labels=None):\n",
    "        \"\"\" Training entrypoint: sample random timesteps & return loss \"\"\"\n",
    "        b, c, h, w = img.shape\n",
    "        assert h == self.image_size and w == self.image_size\n",
    "        t = torch.randint(0, self.num_timesteps, (b, ), device=img.device)\n",
    "        img = self.normalize(img)\n",
    "        return self.p_losses(img, t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb07a306",
   "metadata": {},
   "source": [
    "## Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9923c9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cycle(dl):\n",
    "    while True:\n",
    "        for data in dl:\n",
    "            yield data\n",
    "\n",
    "def num_to_groups(num, divisor):\n",
    "    groups = num // divisor\n",
    "    remainder = num % divisor\n",
    "    arr = [divisor] * groups\n",
    "    if remainder > 0:\n",
    "        arr.append(remainder)\n",
    "    return arr\n",
    "\n",
    "class DiTTrainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        diffusion_model,\n",
    "        data_folder: str,\n",
    "        label_folder: str,\n",
    "        batch_size: int = 16,\n",
    "        lr: float = 1e-4,\n",
    "        num_steps: int = 100000,\n",
    "        grad_accum_steps: int = 1,\n",
    "        ema_decay: float = 0.995,\n",
    "        save_interval: int = 1000,\n",
    "        num_samples: int = 25,\n",
    "        results_folder: str = './results_dit',\n",
    "        use_ddim=False,\n",
    "        num_ddim_steps=50,\n",
    "        eta=0.0\n",
    "    ):\n",
    "        self.accelerator = Accelerator(mixed_precision='fp16')\n",
    "        self.device = self.accelerator.device\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.grad_accum_steps = grad_accum_steps\n",
    "        self.num_steps = num_steps\n",
    "        self.save_interval = save_interval\n",
    "        self.num_samples = num_samples\n",
    "        self.use_ddim = use_ddim\n",
    "        self.num_ddim_steps = num_ddim_steps\n",
    "        self.eta = eta\n",
    "\n",
    "        self.model = diffusion_model.to(self.device)\n",
    "        self.optimizer = Adam(self.model.parameters(), lr=lr)\n",
    "        self.scheduler = CosineAnnealingLR(self.optimizer, T_max=num_steps, eta_min=1e-6)\n",
    "        self.model, self.optimizer = self.accelerator.prepare(self.model, self.optimizer)\n",
    "        self.ema = EMA(self.accelerator.unwrap_model(self.model), beta=ema_decay)\n",
    "\n",
    "        ds = CustomDataset(data_folder, label_folder, diffusion_model.image_size)\n",
    "        dl = DataLoader(ds, batch_size=batch_size, shuffle=True, num_workers=os.cpu_count())\n",
    "        self.dl = cycle(self.accelerator.prepare(dl))\n",
    "\n",
    "        self.results_folder = Path(results_folder)\n",
    "        self.results_folder.mkdir(parents=True, exist_ok=True)\n",
    "        self.step = 0\n",
    "\n",
    "    def save(self, milestone: int):\n",
    "        if not self.accelerator.is_main_process:\n",
    "            return\n",
    "        ckpt = self.results_folder / f'dit_model-{milestone}.pt'\n",
    "        data = {\n",
    "            'step': self.step,\n",
    "            'model': self.accelerator.get_state_dict(self.model),\n",
    "            'optimizer': self.optimizer.state_dict(),\n",
    "            'ema': self.ema.state_dict(),\n",
    "        }\n",
    "        torch.save(data, ckpt)\n",
    "\n",
    "    def load(self, ckpt_path: str):\n",
    "        data = torch.load(ckpt_path, map_location=self.device)\n",
    "        raw_model = self.accelerator.unwrap_model(self.model)\n",
    "        raw_model.load_state_dict(data['model'])\n",
    "        self.optimizer.load_state_dict(data['optimizer'])\n",
    "        self.ema.load_state_dict(data['ema'])\n",
    "        self.step = data['step']\n",
    "\n",
    "    def _sample_and_save(self, milestone: int):\n",
    "        self.ema.ema_model.eval()\n",
    "        batches = num_to_groups(self.num_samples, self.batch_size)\n",
    "        imgs = torch.cat([\n",
    "            self.ema.ema_model.sample(batch_size=n, use_ddim=self.use_ddim, num_ddim_steps=self.num_ddim_steps, eta=self.eta) for n in batches\n",
    "        ], dim=0)\n",
    "        path = self.results_folder / f'dit_sample-{milestone}.png'\n",
    "        vutils.save_image(imgs, path, nrow=int(math.sqrt(self.num_samples)))\n",
    "\n",
    "    def train(self):\n",
    "        pbar = tqdm(total=self.num_steps, initial=self.step, disable=not self.accelerator.is_main_process)\n",
    "        while self.step < self.num_steps:\n",
    "            total_loss = 0.0\n",
    "            for _ in range(self.grad_accum_steps):\n",
    "                imgs, labels = next(self.dl)\n",
    "                imgs = imgs.to(self.device)\n",
    "                with self.accelerator.autocast():\n",
    "                    loss = self.model(imgs) / self.grad_accum_steps\n",
    "                total_loss += loss.item()\n",
    "                self.accelerator.backward(loss)\n",
    "\n",
    "            self.accelerator.clip_grad_norm_(self.model.parameters(), 1.0)\n",
    "            self.optimizer.step()\n",
    "            self.scheduler.step()\n",
    "            self.optimizer.zero_grad()\n",
    "            self.step += 1\n",
    "\n",
    "            if self.accelerator.is_main_process:\n",
    "                self.ema.update()\n",
    "                if self.step % self.save_interval == 0:\n",
    "                    milestone = self.step // self.save_interval\n",
    "                    self._sample_and_save(milestone)\n",
    "                    self.save(milestone)\n",
    "\n",
    "            # if self.step % 10 == 0 and self.accelerator.is_main_process:\n",
    "            #     print(f\"[Step {self.step}] loss: {total_loss:.4f}\")\n",
    "\n",
    "            pbar.set_description(f'loss: {total_loss:.4f}')\n",
    "            pbar.update(1)\n",
    "\n",
    "        if self.accelerator.is_main_process:\n",
    "            print('Training complete.')\n",
    "\n",
    "    def inference(self, total: int = 1000, output_path: str = './submission_dit'):\n",
    "        Path(output_path).mkdir(parents=True, exist_ok=True)\n",
    "        count = 0\n",
    "        batch_size = self.batch_size\n",
    "\n",
    "        with torch.no_grad():\n",
    "            while count < total:\n",
    "                n = min(batch_size, total - count)\n",
    "                if torch.cuda.is_available():\n",
    "                    gc.collect()\n",
    "                    torch.cuda.empty_cache()\n",
    "                imgs = self.ema.ema_model.sample(batch_size=n, use_ddim=self.use_ddim, num_ddim_steps=self.num_ddim_steps, eta=self.eta)\n",
    "                imgs = imgs.cpu()\n",
    "                for img in imgs:\n",
    "                    count += 1\n",
    "                    vutils.save_image(img, f\"{output_path}/{count}.jpg\")\n",
    "                del imgs\n",
    "                if torch.cuda.is_available():\n",
    "                    gc.collect()\n",
    "                    torch.cuda.empty_cache()\n",
    "        print(\"Inference complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6916a5",
   "metadata": {},
   "source": [
    "## Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "640e97a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbed(nn.Module):\n",
    "    def __init__(self, in_channels, embed_dim, patch_size):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Conv2d(in_channels, embed_dim, patch_size, patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x)\n",
    "        x = x.flatten(2).transpose(1, 2) # (B, N_patches, embed_dim)\n",
    "        return x\n",
    "\n",
    "class ConditionEmbed(nn.Module):\n",
    "    def __init__(self, num_classes:int, embed_dim:int):\n",
    "        super().__init__()\n",
    "        self.label_embed = nn.Embedding(num_classes + 1, embed_dim)\n",
    "        nn.init.normal_(self.label_embed.weight, std=0.02)\n",
    "\n",
    "    def forward(self, labels: torch.LongTensor):\n",
    "        return self.label_embed(labels)\n",
    "\n",
    "class AdaLNBlock(nn.Module):\n",
    "    def __init__(self, dim, num_heads, cond_dim):\n",
    "        super().__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(dim, num_heads, batch_first=True)\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        self.feedforward = nn.Sequential(\n",
    "            nn.Linear(dim, dim*4),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(dim*4, dim)\n",
    "        )\n",
    "        self.mlp_gamma = nn.Linear(cond_dim, dim)\n",
    "        self.mlp_beta = nn.Linear(cond_dim, dim)\n",
    "\n",
    "    def forward(self, x, cond):\n",
    "        gamma1, beta1 = self.mlp_gamma(cond), self.mlp_beta(cond)\n",
    "        x_norm = self.norm1(x)\n",
    "        x = x + self.self_attn(x_norm, x_norm, x_norm)[0] * (1+gamma1.unsqueeze(1)) + beta1.unsqueeze(1)\n",
    "\n",
    "        gamma2, beta2 = self.mlp_gamma(cond), self.mlp_beta(cond)\n",
    "        x_norm = self.norm2(x)\n",
    "        x = x + self.feedforward(x_norm) * (1 + gamma2.unsqueeze(1)) + beta2.unsqueeze(1)\n",
    "\n",
    "        return x\n",
    "    \n",
    "class CrossAttentionBlock(nn.Module):\n",
    "    def __init__(self, dim, num_heads, cond_dim):\n",
    "        super().__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(dim, num_heads, batch_first=True)\n",
    "        self.cross_attn = nn.MultiheadAttention(dim, num_heads, batch_first=True)\n",
    "        \n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        self.norm3 = nn.LayerNorm(dim)\n",
    "\n",
    "        self.feedforward = nn.Sequential(\n",
    "            nn.Linear(dim, dim * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(dim * 4, dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, cond_tokens):\n",
    "        # Self-Attention\n",
    "        x = x + self.self_attn(self.norm1(x), self.norm1(x), self.norm1(x))[0]\n",
    "        # Cross-Attention\n",
    "        x = x + self.cross_attn(self.norm2(x), cond_tokens, cond_tokens)[0]\n",
    "        # Feedforward\n",
    "        x = x + self.feedforward(self.norm3(x))\n",
    "        return x\n",
    "    \n",
    "class InContextConditionBlock(nn.Module):\n",
    "    def __init__(self, dim, num_heads):\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention(dim, num_heads, batch_first=True)\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "\n",
    "        self.feedforward = nn.Sequential(\n",
    "            nn.Linear(dim, dim * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(dim * 4, dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, cond_tokens):\n",
    "        # Concatenate condition tokens\n",
    "        x_cat = torch.cat([x, cond_tokens], dim=1)  # (B, N+M, D)\n",
    "        # Apply transformer attention over both\n",
    "        attn_out = self.attn(self.norm1(x_cat), self.norm1(x_cat), self.norm1(x_cat))[0]\n",
    "        x_cat = x_cat + attn_out\n",
    "        x_cat = x_cat + self.feedforward(self.norm2(x_cat))\n",
    "        # Return only the original input portion (not condition)\n",
    "        return x_cat[:, :x.size(1)]\n",
    "    \n",
    "class DiTDiffusionModel(nn.Module):\n",
    "    def __init__(self, patch_size=4, in_channels=3, embed_dim=512, depth=12, num_heads=8, cond_dim=128, num_classes=5):\n",
    "        super().__init__()\n",
    "        self.channels = in_channels\n",
    "        self.num_classes = num_classes\n",
    "        self.patch_embed = PatchEmbed(in_channels, embed_dim, patch_size)\n",
    "        self.condition_embed = ConditionEmbed(num_classes=self.num_classes, embed_dim=embed_dim)\n",
    "        self.blocks = nn.ModuleList([\n",
    "            AdaLNBlock(embed_dim, num_heads, cond_dim) for _ in range(depth)\n",
    "        ])\n",
    "        self.to_output = nn.Linear(embed_dim, in_channels * patch_size * patch_size)\n",
    "\n",
    "    def forward(self, x_latent, t=None, labels=None):\n",
    "        B = x_latent.shape[0]\n",
    "        if labels is None:\n",
    "            labels = torch.full((B,), self.num_classes, device=x_latent.device, dtype=torch.long)\n",
    "        cond_emb = self.condition_embed(labels)\n",
    "        x = self.patch_embed(x_latent)\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x, cond_emb)\n",
    "        return self.to_output(x).transpose(1, 2).view_as(x_latent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3bd4ac",
   "metadata": {},
   "source": [
    "## Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d41f032",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DiTDiffusionModel(\n",
    "    patch_size=2,\n",
    "    in_channels=3,\n",
    "    embed_dim=256,\n",
    "    depth=12,\n",
    "    num_heads=4,\n",
    "    cond_dim=128\n",
    ")\n",
    "\n",
    "diffusion_model = DiTDiffusion(\n",
    "    model=model,\n",
    "    image_size=16,\n",
    "    timesteps=1000,\n",
    "    beta_schedule='linear',\n",
    "    lambda_l1=0.1,\n",
    "    auto_normalize=True\n",
    ")\n",
    "\n",
    "trainer = DiTTrainer(\n",
    "    diffusion_model=diffusion_model,\n",
    "    data_folder='./sprites.npy',\n",
    "    label_folder='./sprites_labels.npy',\n",
    "    batch_size=32,\n",
    "    lr=1e-4,\n",
    "    num_steps=100000,\n",
    "    save_interval=500,\n",
    "    num_samples=36,\n",
    "    results_folder='./results_dit',\n",
    "    use_ddim=False,\n",
    "    num_ddim_steps=50,\n",
    "    eta=0.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7521c84f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14ac255f15fb4a498aa65b78c29a399b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (32x256 and 128x256)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 95\u001b[0m, in \u001b[0;36mDiTTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     93\u001b[0m imgs \u001b[38;5;241m=\u001b[39m imgs\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mautocast():\n\u001b[0;32m---> 95\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrad_accum_steps\n\u001b[1;32m     96\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mbackward(loss)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py:818\u001b[0m, in \u001b[0;36mconvert_outputs_to_fp32.<locals>.forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 818\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py:806\u001b[0m, in \u001b[0;36mConvertOutputsToFp32.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    805\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 806\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m convert_to_fp32(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/amp/autocast_mode.py:44\u001b[0m, in \u001b[0;36mautocast_decorator.<locals>.decorate_autocast\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_autocast\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m autocast_instance:\n\u001b[0;32m---> 44\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 245\u001b[0m, in \u001b[0;36mDiTDiffusion.forward\u001b[0;34m(self, img, labels)\u001b[0m\n\u001b[1;32m    243\u001b[0m t \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps, (b, ), device\u001b[38;5;241m=\u001b[39mimg\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    244\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnormalize(img)\n\u001b[0;32m--> 245\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mp_losses\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 226\u001b[0m, in \u001b[0;36mDiTDiffusion.p_losses\u001b[0;34m(self, x_start, t, noise)\u001b[0m\n\u001b[1;32m    224\u001b[0m     noise \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn_like(x_start)\n\u001b[1;32m    225\u001b[0m x_noisy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_sample(x_start, t, noise)\n\u001b[0;32m--> 226\u001b[0m pred_noise \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_noisy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    227\u001b[0m mse_loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmse_loss(pred_noise, noise, reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    228\u001b[0m mse_loss \u001b[38;5;241m=\u001b[39m mse_loss\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, mse_loss\u001b[38;5;241m.\u001b[39mndim))) \u001b[38;5;66;03m# mean over c, h, w\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[5], line 112\u001b[0m, in \u001b[0;36mDiTDiffusionModel.forward\u001b[0;34m(self, x_latent, t, labels)\u001b[0m\n\u001b[1;32m    110\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpatch_embed(x_latent)\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks:\n\u001b[0;32m--> 112\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mblk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcond_emb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_output(x)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mview_as(x_latent)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[5], line 35\u001b[0m, in \u001b[0;36mAdaLNBlock.forward\u001b[0;34m(self, x, cond)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, cond):\n\u001b[0;32m---> 35\u001b[0m     gamma1, beta1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp_gamma\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcond\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp_beta(cond)\n\u001b[1;32m     36\u001b[0m     x_norm \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(x)\n\u001b[1;32m     37\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn(x_norm, x_norm, x_norm)[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m+\u001b[39mgamma1\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)) \u001b[38;5;241m+\u001b[39m beta1\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (32x256 and 128x256)"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98011f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.load('./results_dit/dit_model-20.pt')\n",
    "trainer.inference(total=1000, output_path='./samples')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
